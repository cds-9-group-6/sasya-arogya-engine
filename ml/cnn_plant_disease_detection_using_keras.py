# -*- coding: utf-8 -*-
"""CNN_Plant_Disease_Detection_using_Keras.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Tca9CJYh1_Fqx5iRwFBp_Z90NXqNmfN4

# Download the Plant Dataset
"""

!wget https://data.mendeley.com/public-files/datasets/tywbtsjrjv/files/b4e3a32f-c0bd-4060-81e9-6144231f2520/file_downloaded -O plant_disease_dataset.zip

!unzip plant_disease_dataset.zip

!pip install keras
!pip install -U scikit-learn
!pip install tensorflow

"""Import neccessary packages"""

import numpy as np
import pickle
import cv2
import os
from keras.callbacks import Callback
from os import listdir

import tensorflow as tf
import math

from sklearn.preprocessing import LabelBinarizer, MultiLabelBinarizer
from keras.models import Sequential
from keras.layers import BatchNormalization, Conv2D, MaxPooling2D, Activation, Flatten, Dropout, Dense
from keras import backend as K
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.utils import img_to_array
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt

from tensorflow.keras.callbacks import ModelCheckpoint, Callback
from tensorflow.keras.utils import to_categorical
from sklearn.model_selection import train_test_split

import random # Import the random library
from skimage.util import random_noise # Import random_noise for adding noise

EPOCHS = 25
INIT_LR = 0.01
BS = 32
default_image_size = tuple((64, 64))
image_size = 0
directory_root = '/content/Plant_leave_diseases_dataset_with_augmentation'
width=64
height=64
depth=3

"""Function to convert images to array"""

def convert_image_to_array(image_dir):
    try:
        image = cv2.imread(image_dir)
        if image is not None :
            image = cv2.resize(image, default_image_size)
            return img_to_array(image)
        else :
            return np.array([])
    except Exception as e:
        print(f"Error : {e}")
        return None

"""Fetch images from directory"""

def get_disease_name (file_name):
    #file_name = "Apple___Apple_scab"

    # Split the string by "___"
    parts = file_name.split("___")

    # The first part is the plant name
    plant_name = parts[0]

    if (len(parts)>1):
     # The second part is the disease name
      disease_name = parts[1]
    else:
      disease_name = "Unknown"

    return disease_name

image_list, label_list = [], []
try:
    print("[INFO] Loading images ...")
    root_dir = listdir(directory_root)
    for directory in root_dir :
        # remove .DS_Store from list
        if directory == ".DS_Store" :
            root_dir.remove(directory)

    for plant_folder in root_dir :

        print(f"[INFO] Processing {plant_folder} ...")
        plant_disease_image_list = listdir(f"{directory_root}/{plant_folder}/")

        plant_disease_folder = get_disease_name(plant_folder)

        for single_plant_disease_image in plant_disease_image_list :
            if single_plant_disease_image == ".DS_Store" :
                plant_disease_image_list.remove(single_plant_disease_image)

        for image in plant_disease_image_list:
            image_directory = f"{directory_root}/{plant_folder}/{image}"
            if image_directory.endswith(".jpg") == True or image_directory.endswith(".JPG") == True:
                image_list.append(convert_image_to_array(image_directory))
                label_list.append(plant_disease_folder)
    print("[INFO] Image loading completed")

    if not image_list or not label_list:
        print("[ERROR] No images or labels were loaded. Please check the directory and image files.")

except Exception as e:
    print(f"Error : {e}")

"""Get Size of Processed Image"""

image_size = len(image_list)

image_size

"""Transform Image Labels uisng [Scikit Learn](http://scikit-learn.org/)'s LabelBinarizer"""

label_binarizer = LabelBinarizer()
image_labels = label_binarizer.fit_transform(label_list)
pickle.dump(label_binarizer,open('label_transform.pkl', 'wb'))
n_classes = len(label_binarizer.classes_)

"""Print the classes"""

print(label_binarizer.classes_)

np_image_list = np.array(image_list, dtype=np.float16) / 255.0



"""# Data Preprocessing and Augmentation

Create the augmented datset of 360K images from fixed dataset of 60K images by doing shifting, rotation, flip, shear and noise addition. We made sure there is randomness in generating augmented data
"""



"""## Data augmentation



"""

import os
import cv2
import numpy as np
from skimage.util import random_noise
import random # Ensure random is imported

# Number of augmented versions per image
augmentation_factor = 4 #5
batch_size = 1000 # Define a reasonable batch size
num_batches = (len(np_image_list) + batch_size - 1) // batch_size # Calculate the number of batches

# Define directories to save augmented images and labels
augmented_images_dir = '/content/random_augmented_images' # Changed directory name
augmented_labels_dir = '/content/random_augmented_labels' # Changed directory name

# Create directories if they don't exist
os.makedirs(augmented_images_dir, exist_ok=True)
os.makedirs(augmented_labels_dir, exist_ok=True)

print(f"[INFO] Generating random augmented dataset in batches (each image augmented {augmentation_factor} times) and saving as JPG files...")

for batch_idx in range(num_batches):
    print(f"[INFO] Processing batch {batch_idx + 1}/{num_batches}...")

    start_idx = batch_idx * batch_size
    end_idx = min((batch_idx + 1) * batch_size, len(np_image_list))

    batch_images = np_image_list[start_idx:end_idx]
    batch_labels = image_labels[start_idx:end_idx]

    # Define filename for saving labels for the current batch
    augmented_labels_filename = os.path.join(augmented_labels_dir, f'random_augmented_labels_batch_{batch_idx}.txt') # Changed filename

    # Open the labels file for writing
    with open(augmented_labels_filename, 'w') as label_file:

        # Iterate over the original images in the current batch and generate augmented versions
        for i in range(len(batch_images)):
            image = batch_images[i]
            label = batch_labels[i]

            # Save the original image and label
            image_filename = os.path.join(augmented_images_dir, f'batch_{batch_idx}_original_{i}.jpg')
            # Convert image back to uint8 for saving
            cv2.imwrite(image_filename, (image * 255).astype(np.uint8))
            label_file.write(f'batch_{batch_idx}_original_{i}.jpg,{np.argmax(label)}\n') # Save label (assuming one-hot encoding)


            for aug_factor_idx in range(augmentation_factor):
                augmented_image = image.copy()

                # Ensure image is in uint8 format for OpenCV transformations
                augmented_image_uint8 = (augmented_image * 255).astype(np.uint8)

                rows, cols, _ = augmented_image_uint8.shape

                # 1. Rotation (random extent from 10-45 degrees)
                #angle = random.uniform(10, 45)
                #M_rotate = cv2.getRotationMatrix2D(((cols-1)/2.0,(rows-1)/2.0),angle,1)
                #augmented_image_uint8 = cv2.warpAffine(augmented_image_uint8, M_rotate, (cols, rows), borderMode=cv2.BORDER_REPLICATE) # Use BORDER_REPLICATE to fill

                # 2. Shift (random range of 10-30%)
                shift_range_x = random.uniform(0.1, 0.3) * cols
                shift_range_y = random.uniform(0.1, 0.3) * rows
                tx = random.uniform(-shift_range_x, shift_range_x) # Random horizontal shift
                ty = random.uniform(-shift_range_y, shift_range_y) # Random vertical shift
                M_shift = np.float32([[1, 0, tx], [0, 1, ty]])
                #augmented_image_uint8 = cv2.warpAffine(augmented_image_uint8, M_shift, (cols, rows), borderMode=cv2.BORDER_REPLICATE) # Use BORDER_REPLICATE to fill
                augmented_image_uint8 = cv2.warpAffine(augmented_image_uint8, M_shift, (cols, rows), borderMode=cv2.BORDER_CONSTANT, borderValue=(0, 0, 0)) # Use BORDER_REPLICATE to fill


                # 3. Flip (random horizontal or vertical flip)
                flip_code = random.choice([-1, 0, 1]) # -1: both, 0: vertical, 1: horizontal
                augmented_image_uint8 = cv2.flip(augmented_image_uint8, flip_code)


                # 4. Shear (random factor in range of 0.2-0.4)
                shear_factor = random.uniform(0.2, 0.4)
                M_shear = np.float32([[1, shear_factor, 0], [0, 1, 0]]) # Horizontal shear example
                # Apply shear and then resize back to original dimensions
                #augmented_image_uint8 = cv2.warpAffine(augmented_image_uint8, M_shear, (cols + int(cols*shear_factor), rows), borderMode=cv2.BORDER_REPLICATE) # Use BORDER_REPLICATE to fill
                augmented_image_uint8 = cv2.warpAffine(augmented_image_uint8, M_shear, (cols + int(cols*shear_factor), rows), borderMode=cv2.BORDER_CONSTANT, borderValue=(0, 0, 0))
                augmented_image_uint8 = cv2.resize(augmented_image_uint8, (cols, rows))


                # Convert back to float64 for noise addition
                augmented_image_float64 = augmented_image_uint8.astype(np.float64) / 255.0

                # 5. Add Noise (Gaussian noise - randomly apply)
                if random.random() > 0.5: # Randomly apply noise with 50% probability
                    noisy_image = random_noise(augmented_image_float64, mode='gaussian', var=random.uniform(0.005, 0.02)) # Random noise variance
                    augmented_image = np.clip(noisy_image, 0, 1) # Clip values to be within the valid range [0, 1]
                else:
                    augmented_image = augmented_image_float64 # Use the image without noise


                # Save the augmented image and label
                image_filename = os.path.join(augmented_images_dir, f'batch_{batch_idx}_aug_{i}_{aug_factor_idx}.jpg')
                 # Convert augmented image back to uint8 for saving
                cv2.imwrite(image_filename, (augmented_image * 255).astype(np.uint8))
                label_file.write(f'batch_{batch_idx}_aug_{i}_{aug_factor_idx}.jpg,{np.argmax(label)}\n') # Save label (assuming one-hot encoding)


    print(f"[INFO] Augmented dataset for batch {batch_idx + 1} saved successfully.")


print("[INFO] Random augmented dataset generation completed across all batches.")

"""## Generate the Zip dataset files and store


"""

import os
import zipfile

# Define the directories where the augmented data is saved
augmented_images_dir = '/content/random_augmented_images'
augmented_labels_dir = '/content/random_augmented_labels'
zip_filename = 'bb_random_augmented_dataset.zip' # Define the name for the zip file

print(f"[INFO] Zipping augmented images and labels into {zip_filename}...")
index = 0
# Create a zip file
with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:
    # Add image files to the zip file
    for root, _, files in os.walk(augmented_images_dir):
        for file in files:
            file_path = os.path.join(root, file)
            zipf.write(file_path, os.path.relpath(file_path, augmented_images_dir)) # Add file with relative path

    # Add label files to the zip file
    for root, _, files in os.walk(augmented_labels_dir):
        for file in files:
            file_path = os.path.join(root, file)
            #print("Labels File path", file_path)
            zipf.write(file_path, os.path.relpath(file_path, augmented_labels_dir)) # Add file with relative path


print("[INFO] Zipping completed.")
print(f"Created zip file: {zip_filename}")

#Clean temprary generated files

import os
import shutil

# Define the directory to remove
extract_dir = '/content/extracted_augmented_data'

#extract_dir = '/content/random_augmented_labels'

print(f"[INFO] Removing extracted directory: {extract_dir}...")

# Remove the directory and its contents
if os.path.exists(extract_dir):
    shutil.rmtree(extract_dir)
    print("[INFO] Extracted directory removed.")
else:
    print("[INFO] Extracted directory does not exist.")

"""# Load the dataset

## Mount the google drive, download the zip file, extract files.
"""

from google.colab import drive

# Mount Google Drive
drive.mount('/content/drive')

import os
import zipfile
import cv2
import numpy as np

def extract_zip(zip_file_path, extract_dir):
    """Extracts a zip file to a specified directory."""
    print(f"[INFO] Extracting data from {zip_file_path} to {extract_dir}...")
    os.makedirs(extract_dir, exist_ok=True) # Create extract directory if it doesn't exist
    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:
        zip_ref.extractall(extract_dir)
    print("[INFO] Extraction completed.")

# Define the path to the zip file and the directory to extract to
zip_file_path = '/content/drive/MyDrive/PlantDataset/bb_random_augmented_dataset.zip'
extract_dir = '/content/extracted_augmented_data' # Directory to extract the zip file contents

# --- Example usage of the new functions ---

# 1. Extract the zip file
extract_zip(zip_file_path, extract_dir)

print("Extraced ZIP successfuly")

label_list = ['Apple_scab','Bacterial_spot','Black_rot','Cedar_apple_rust',
 'Cercospora_leaf_spot Gray_leaf_spot', 'Common_rust' ,'Early_blight',
 'Esca_(Black_Measles)','Haunglongbing_(Citrus_greening)' ,'Late_blight',
 'Leaf_Mold', 'Leaf_blight_(Isariopsis_Leaf_Spot)', 'Leaf_scorch',
 'Northern_Leaf_Blight' ,'Powdery_mildew' ,'Septoria_leaf_spot',
 'Spider_mites Two-spotted_spider_mite', 'Target_Spot',
 'Tomato_Yellow_Leaf_Curl_Virus', 'Tomato_mosaic_virus' ,'Unknown', 'healthy']

label_binarizer = LabelBinarizer()
image_labels = label_binarizer.fit_transform(label_list)
pickle.dump(label_binarizer,open('label_transform.pkl', 'wb'))
n_classes = len(label_binarizer.classes_)
print(label_binarizer.classes_)

#Callback to stop the model if accuracy is more than 90% for both training and validation

class AccuracyThresholdCallback(Callback):
    def __init__(self, acc_threshold=0.9, val_acc_threshold=0.9):
        super(AccuracyThresholdCallback, self).__init__()
        self.acc_threshold = acc_threshold
        self.val_acc_threshold = val_acc_threshold

    def on_epoch_end(self, epoch, logs=None):
        if logs is None:
            logs = {}
        acc = logs.get('accuracy')
        val_acc = logs.get('val_accuracy')
        if acc is not None and val_acc is not None:
            if acc >= self.acc_threshold and val_acc >= self.val_acc_threshold:
                print(f"\nStopping training as accuracy ({acc:.4f}) and validation accuracy ({val_acc:.4f}) reached the threshold.")
                self.model.stop_training = True

label_binarizer.transform(label_list)

image_labels[3]

EPOCHS = 25
INIT_LR = 0.01
BS = 32
default_image_size = tuple((64, 64))
image_size = 0
#directory_root = '/content/Plant_leave_diseases_dataset_with_augmentation'
width=64
height=64
depth=3

"""# A. CNN Model for entire Dataset"""



cnn_model = Sequential()
inputShape = (height, width, depth)
chanDim = -1
if K.image_data_format() == "channels_first":
    inputShape = (depth, height, width)
    chanDim = 1
cnn_model.add(Conv2D(32, (3, 3), padding="same",input_shape=inputShape))
cnn_model.add(Activation("relu"))
cnn_model.add(BatchNormalization(axis=chanDim))
cnn_model.add(MaxPooling2D(pool_size=(3, 3)))
cnn_model.add(Dropout(0.25))
cnn_model.add(Conv2D(64, (3, 3), padding="same"))
cnn_model.add(Activation("relu"))
cnn_model.add(BatchNormalization(axis=chanDim))
cnn_model.add(Conv2D(64, (3, 3), padding="same"))
cnn_model.add(Activation("relu"))
cnn_model.add(BatchNormalization(axis=chanDim))
cnn_model.add(MaxPooling2D(pool_size=(2, 2)))
cnn_model.add(Dropout(0.25))
cnn_model.add(Conv2D(128, (3, 3), padding="same"))
cnn_model.add(Activation("relu"))
cnn_model.add(BatchNormalization(axis=chanDim))
cnn_model.add(Conv2D(128, (3, 3), padding="same"))
cnn_model.add(Activation("relu"))
cnn_model.add(BatchNormalization(axis=chanDim))
cnn_model.add(MaxPooling2D(pool_size=(2, 2)))
cnn_model.add(Dropout(0.25))
cnn_model.add(Flatten())
cnn_model.add(Dense(1024))
cnn_model.add(Activation("relu"))
cnn_model.add(BatchNormalization())
cnn_model.add(Dropout(0.5))
cnn_model.add(Dense(n_classes))
cnn_model.add(Activation("softmax"))
cnn_model.summary()

opt = Adam(learning_rate=INIT_LR, decay=INIT_LR / EPOCHS)
# distribution
cnn_model.compile(loss="categorical_crossentropy", optimizer=opt,metrics=["accuracy"])
# train the network
print("[INFO] training network...")

"""# Data generators prep for model training
Change the model training code to use separate data generators for training and validation data, split from the augmented dataset, and ensure the model training stops when both training and validation accuracy exceed 90%.

## Create separate data generators for training and validation

### Subtask:
Instantiate two `AugmentedDataGenerator` objects, one pointing to the training data directory and the other to the validation data directory.

**Reasoning**:
Instantiate two `AugmentedDataGenerator` objects, one for training and one for validation, using the previously defined directories and parameters.
"""

import tensorflow as tf
import numpy as np
import os
import cv2
import math
from sklearn.model_selection import train_test_split # Import train_test_split

class SplitAugmentedDataGenerator(tf.keras.utils.Sequence):
    """
    Custom data generator that loads augmented images and labels in batches
    and performs train/validation split internally based on file paths.
    """
    def __init__(self, extracted_base_dir, label_binarizer, batch_size=32, shuffle=True, validation_split=0.2, is_validation=False):
        self.extracted_base_dir = extracted_base_dir
        self.label_binarizer = label_binarizer
        self.batch_size = batch_size
        self.shuffle = shuffle
        self.validation_split = validation_split
        self.is_validation = is_validation

        self.extracted_images_dir = os.path.join(self.extracted_base_dir, 'random_augmented_images')
        self.extracted_labels_dir = os.path.join(self.extracted_base_dir, 'random_augmented_labels')

        # Load all image file paths and corresponding labels from the label files
        all_image_files = []
        all_labels = []

        label_files = sorted([f for f in os.listdir(self.extracted_labels_dir) if f.endswith('.txt')])
        for label_file_name in label_files:
            label_file_path = os.path.join(self.extracted_labels_dir, label_file_name)
            with open(label_file_path, 'r') as f:
                for line in f:
                    parts = line.strip().split(',')
                    if len(parts) == 2:
                        image_filename = parts[0]
                        label = int(parts[1])
                        all_image_files.append(os.path.join(self.extracted_images_dir, image_filename))
                        all_labels.append(label)

        # Perform the train/validation split on the file paths and labels
        # We use a fixed random state to ensure the split is the same for both generators
        if not is_validation:
            self.image_files, _, self.labels, _ = train_test_split(
                all_image_files, all_labels, test_size=validation_split, random_state=42, shuffle=self.shuffle
            )
        else:
            _, self.image_files, _, self.labels = train_test_split(
                all_image_files, all_labels, test_size=validation_split, random_state=42, shuffle=self.shuffle
            )


        self.on_epoch_end() # Initial shuffling if needed

    def __len__(self):
        """Denotes the number of batches per epoch."""
        return math.ceil(len(self.image_files) / self.batch_size)

    def __getitem__(self, index):
        """Generate one batch of data."""
        # Generate indexes of the batch
        indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]

        # Find list of IDs for the batch
        batch_image_files = [self.image_files[k] for k in indexes]
        batch_labels = [self.labels[k] for k in indexes]

        # Generate data
        X, y = self.__data_generation(batch_image_files, batch_labels)

        return X, y

    def on_epoch_end(self):
        """Updates indexes after each epoch."""
        self.indexes = np.arange(len(self.image_files))
        if self.shuffle == True:
            np.random.shuffle(self.indexes)

    def __data_generation(self, batch_image_files, batch_labels):
        """Generates data containing batch_size samples."""
        # Initialization
        # Assuming all images have the same dimensions (e.g., 64x64x3)
        img_height, img_width, img_depth = 64, 64, 3 # Use your defined image shape
        X = np.empty((len(batch_image_files), img_height, img_width, img_depth), dtype=np.float16)
        y = np.empty((len(batch_image_files), len(self.label_binarizer.classes_)), dtype=np.float32) # Use float32 for one-hot encoding

        # Generate data
        for i, (image_file, label) in enumerate(zip(batch_image_files, batch_labels)):
            # Load image
            image = cv2.imread(image_file)
            if image is not None:
                # Convert image to float and normalize
                image = image.astype(np.float16) / 255.0
                X[i,] = image

                # Convert label to one-hot encoding
                #y[i,] = self.label_binarizer.transform([label])[0]
                y[i,] = self.label_binarizer.transform([label_binarizer.classes_[label]])[0]

            else:
                print(f"Warning: Could not load image {image_file} in data generator. Filling with zeros.")
                # Handle missing images - fill with zeros or a placeholder
                X[i,] = np.zeros((img_height, img_width, img_depth), dtype=np.float16)
                y[i,] = np.zeros(len(self.label_binarizer.classes_), dtype=np.float32)


        return X, y

# --- Example Usage ---
# Assuming you have extracted the zip file to '/content/extracted_augmented_data'
# and have the label_binarizer object available from previous steps

# Define the base directory where the zip extracted its contents
extracted_base_dir = '/content/extracted_augmented_data/bb_random_augmented_dataset' # Use the directory containing image and label folders

# Make sure 'label_binarizer' and 'BS' (batch size) are defined in previous cells

if 'label_binarizer' in locals() and 'BS' in locals():
    # Create the training data generator
    train_generator = SplitAugmentedDataGenerator(
        extracted_base_dir=extracted_base_dir,
        label_binarizer=label_binarizer,
        batch_size=BS,
        shuffle=True,
        validation_split=0.2, # Define your desired validation split
        is_validation=False
    )
    print("[INFO] Training data generator created.")
    print(f"Number of training batches per epoch: {len(train_generator)}")
    print(f"Total number of training images: {len(train_generator.image_files)}")


    # Create the validation data generator
    validation_generator = SplitAugmentedDataGenerator(
        extracted_base_dir=extracted_base_dir,
        label_binarizer=label_binarizer,
        batch_size=BS,
        shuffle=False, # No need to shuffle validation data
        validation_split=0.2, # Use the same validation split
        is_validation=True # Set to True for the validation generator
    )
    print("[INFO] Validation data generator created.")
    print(f"Number of validation batches per epoch: {len(validation_generator)}")
    print(f"Total number of validation images: {len(validation_generator.image_files)}")

else:
    print("[ERROR] 'label_binarizer' or 'BS' not found. Please run the cells that define them.")

# Define the accuracy callback for early stopping
accuracy_callback = AccuracyThresholdCallback(acc_threshold=0.9, val_acc_threshold=0.9)
print("[INFO] AccuracyThresholdCallback created.")

# Define ModelCheckpoint callback
checkpoint_filepath = '/tmp/checkpoint/best_model.h5' # Define checkpoint file path
model_checkpoint_callback = ModelCheckpoint(
    filepath=checkpoint_filepath,
    save_best_only=True, # Save only the best model based on the monitored metric
    monitor='val_accuracy', # Monitor validation accuracy
    mode='max', # Save when validation accuracy is maximum
    verbose=1
)
print(f"[INFO] ModelCheckpoint callback created. Best model will be saved to {checkpoint_filepath}")


# Train the attention CNN model using the subset data generators
# Assumes attention_cnn_model is defined and compiled
if 'cnn_model' in locals() and train_generator is not None and validation_generator is not None:
    print("[INFO] Training  CNN model on the entire subset...")

    # Add the ModelCheckpoint callback to the callbacks list
    callbacks_list = [callback for callback in [accuracy_callback, model_checkpoint_callback] if callback is not None]

    history = cnn_model.fit(
        train_generator,
        validation_data=validation_generator,
        epochs=EPOCHS, # Assuming EPOCHS is defined
        verbose=1,
        callbacks=callbacks_list # Use the updated callbacks list
    )
    print("[INFO] Attention CNN model training on subset completed.")

    # Save the final model after training
    final_model_filepath = 'cnn_model_final.keras'
    cnn_model.save(final_model_filepath)
    print(f"[INFO] Final model saved to {final_model_filepath}")

else:
    print("[ERROR]  CNN model or data generators not available for training.")

"""# Model Training

# B. CNN with Attention for entire Dataset
"""

from keras.models import Sequential
from keras.layers import Conv2D, MaxPooling2D, Activation, BatchNormalization, Dropout
from keras import backend as K

from keras.layers import Attention, GlobalAveragePooling2D, Dense, Input, Layer
import tensorflow as tf
from keras.models import Model
from tensorflow.keras.optimizers import Adam # Import Adam optimizer

# Define the CNN base model architecture (same as before)
cnn_base = Sequential()
inputShape = (height, width, depth) # Assuming height, width, depth are defined
chanDim = -1
if K.image_data_format() == "channels_first":
    inputShape = (depth, height, width)
    chanDim = 1

cnn_base.add(Conv2D(32, (3, 3), padding="same", input_shape=inputShape))
cnn_base.add(Activation("relu"))
cnn_base.add(BatchNormalization(axis=chanDim))
cnn_base.add(MaxPooling2D(pool_size=(3, 3)))
cnn_base.add(Dropout(0.25))

cnn_base.add(Conv2D(64, (3, 3), padding="same"))
cnn_base.add(Activation("relu"))
cnn_base.add(BatchNormalization(axis=chanDim))
cnn_base.add(Conv2D(64, (3, 3), padding="same"))
cnn_base.add(Activation("relu"))
cnn_base.add(BatchNormalization(axis=chanDim))
cnn_base.add(MaxPooling2D(pool_size=(2, 2)))
cnn_base.add(Dropout(0.25))

cnn_base.add(Conv2D(128, (3, 3), padding="same"))
cnn_base.add(Activation("relu"))
cnn_base.add(BatchNormalization(axis=chanDim))
cnn_base.add(Conv2D(128, (3, 3), padding="same"))
cnn_base.add(Activation("relu"))
cnn_base.add(BatchNormalization(axis=chanDim))
cnn_base.add(MaxPooling2D(pool_size=(2, 2)))
cnn_base.add(Dropout(0.25))

print("CNN base model created successfully.")
cnn_base.summary()

# Create a custom Keras Layer for reshaping (same as before)
class ReshapeLayer(Layer):
    def __init__(self, target_shape, **kwargs):
        super(ReshapeLayer, self).__init__(**kwargs)
        self.target_shape = target_shape

    def call(self, inputs):
        return tf.reshape(inputs, self.target_shape)

# Create the attention CNN model (same as before)
inputs = Input(shape=(height, width, depth)) # Assuming height, width, depth are defined
cnn_output = cnn_base(inputs)
batch_size_tensor, h, w, c = cnn_output.shape # Use a different variable name for batch size

cnn_output_reshaped = ReshapeLayer(target_shape=(-1, h * w, c))(cnn_output)
attention_output = Attention()([cnn_output_reshaped, cnn_output_reshaped])
attention_output_reshaped_spatial = ReshapeLayer(target_shape=(-1, h, w, c))(attention_output)
x = GlobalAveragePooling2D()(attention_output_reshaped_spatial)
outputs = Dense(n_classes, activation='softmax')(x) # Assuming n_classes is defined

attention_cnn_model = Model(inputs=inputs, outputs=outputs)

print("CNN model with Attention layer created successfully.")
attention_cnn_model.summary()

# Compile the attention CNN model (using parameters from previous cells)
opt = Adam(learning_rate=INIT_LR, decay=INIT_LR / EPOCHS) # Assuming INIT_LR and EPOCHS are defined
attention_cnn_model.compile(loss="categorical_crossentropy", optimizer=opt, metrics=["accuracy"])

print("[INFO] Attention CNN model compiled.")

import tensorflow as tf
import numpy as np
import os
import cv2
import math
from sklearn.model_selection import train_test_split # Import train_test_split

class SplitAugmentedDataGenerator(tf.keras.utils.Sequence):
    """
    Custom data generator that loads augmented images and labels in batches
    and performs train/validation split internally based on file paths.
    """
    def __init__(self, extracted_base_dir, label_binarizer, batch_size=32, shuffle=True, validation_split=0.2, is_validation=False):
        self.extracted_base_dir = extracted_base_dir
        self.label_binarizer = label_binarizer
        self.batch_size = batch_size
        self.shuffle = shuffle
        self.validation_split = validation_split
        self.is_validation = is_validation

        self.extracted_images_dir = os.path.join(self.extracted_base_dir, 'random_augmented_images')
        self.extracted_labels_dir = os.path.join(self.extracted_base_dir, 'random_augmented_labels')

        # Load all image file paths and corresponding labels from the label files
        all_image_files = []
        all_labels = []

        label_files = sorted([f for f in os.listdir(self.extracted_labels_dir) if f.endswith('.txt')])
        for label_file_name in label_files:
            label_file_path = os.path.join(self.extracted_labels_dir, label_file_name)
            with open(label_file_path, 'r') as f:
                for line in f:
                    parts = line.strip().split(',')
                    if len(parts) == 2:
                        image_filename = parts[0]
                        label = int(parts[1])
                        all_image_files.append(os.path.join(self.extracted_images_dir, image_filename))
                        all_labels.append(label)

        # Perform the train/validation split on the file paths and labels
        # We use a fixed random state to ensure the split is the same for both generators
        if not is_validation:
            self.image_files, _, self.labels, _ = train_test_split(
                all_image_files, all_labels, test_size=validation_split, random_state=42, shuffle=self.shuffle
            )
        else:
            _, self.image_files, _, self.labels = train_test_split(
                all_image_files, all_labels, test_size=validation_split, random_state=42, shuffle=self.shuffle
            )


        self.on_epoch_end() # Initial shuffling if needed

    def __len__(self):
        """Denotes the number of batches per epoch."""
        return math.ceil(len(self.image_files) / self.batch_size)

    def __getitem__(self, index):
        """Generate one batch of data."""
        # Generate indexes of the batch
        indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]

        # Find list of IDs for the batch
        batch_image_files = [self.image_files[k] for k in indexes]
        batch_labels = [self.labels[k] for k in indexes]

        # Generate data
        X, y = self.__data_generation(batch_image_files, batch_labels)

        return X, y

    def on_epoch_end(self):
        """Updates indexes after each epoch."""
        self.indexes = np.arange(len(self.image_files))
        if self.shuffle == True:
            np.random.shuffle(self.indexes)

    def __data_generation(self, batch_image_files, batch_labels):
        """Generates data containing batch_size samples."""
        # Initialization
        # Assuming all images have the same dimensions (e.g., 64x64x3)
        img_height, img_width, img_depth = 64, 64, 3 # Use your defined image shape
        X = np.empty((len(batch_image_files), img_height, img_width, img_depth), dtype=np.float16)
        y = np.empty((len(batch_image_files), len(self.label_binarizer.classes_)), dtype=np.float32) # Use float32 for one-hot encoding

        # Generate data
        for i, (image_file, label) in enumerate(zip(batch_image_files, batch_labels)):
            # Load image
            image = cv2.imread(image_file)
            if image is not None:
                # Convert image to float and normalize
                image = image.astype(np.float16) / 255.0
                X[i,] = image

                # Convert label to one-hot encoding
                #y[i,] = self.label_binarizer.transform([label])[0]
                y[i,] = self.label_binarizer.transform([label_binarizer.classes_[label]])[0]

            else:
                print(f"Warning: Could not load image {image_file} in data generator. Filling with zeros.")
                # Handle missing images - fill with zeros or a placeholder
                X[i,] = np.zeros((img_height, img_width, img_depth), dtype=np.float16)
                y[i,] = np.zeros(len(self.label_binarizer.classes_), dtype=np.float32)


        return X, y

# --- Example Usage ---
# Assuming you have extracted the zip file to '/content/extracted_augmented_data'
# and have the label_binarizer object available from previous steps

# Define the base directory where the zip extracted its contents
extracted_base_dir = '/content/extracted_augmented_data/bb_random_augmented_dataset' # Use the directory containing image and label folders

# Make sure 'label_binarizer' and 'BS' (batch size) are defined in previous cells

if 'label_binarizer' in locals() and 'BS' in locals():
    # Create the training data generator
    train_generator = SplitAugmentedDataGenerator(
        extracted_base_dir=extracted_base_dir,
        label_binarizer=label_binarizer,
        batch_size=BS,
        shuffle=True,
        validation_split=0.2, # Define your desired validation split
        is_validation=False
    )
    print("[INFO] Training data generator created.")
    print(f"Number of training batches per epoch: {len(train_generator)}")
    print(f"Total number of training images: {len(train_generator.image_files)}")


    # Create the validation data generator
    validation_generator = SplitAugmentedDataGenerator(
        extracted_base_dir=extracted_base_dir,
        label_binarizer=label_binarizer,
        batch_size=BS,
        shuffle=False, # No need to shuffle validation data
        validation_split=0.2, # Use the same validation split
        is_validation=True # Set to True for the validation generator
    )
    print("[INFO] Validation data generator created.")
    print(f"Number of validation batches per epoch: {len(validation_generator)}")
    print(f"Total number of validation images: {len(validation_generator.image_files)}")

else:
    print("[ERROR] 'label_binarizer' or 'BS' not found. Please run the cells that define them.")

# Define the accuracy callback for early stopping
accuracy_callback = AccuracyThresholdCallback(acc_threshold=0.9, val_acc_threshold=0.9)
print("[INFO] AccuracyThresholdCallback created.")

# Define ModelCheckpoint callback
checkpoint_filepath = '/tmp/checkpoint/best_model.h5' # Define checkpoint file path
model_checkpoint_callback = ModelCheckpoint(
    filepath=checkpoint_filepath,
    save_best_only=True, # Save only the best model based on the monitored metric
    monitor='val_accuracy', # Monitor validation accuracy
    mode='max', # Save when validation accuracy is maximum
    verbose=1
)
print(f"[INFO] ModelCheckpoint callback created. Best model will be saved to {checkpoint_filepath}")


# Train the attention CNN model using the subset data generators
# Assumes attention_cnn_model is defined and compiled
if 'attention_cnn_model' in locals() and train_generator is not None and validation_generator is not None:
    print("[INFO] Training  Attention CNN model on the entire subset...")

    # Add the ModelCheckpoint callback to the callbacks list
    callbacks_list = [callback for callback in [accuracy_callback, model_checkpoint_callback] if callback is not None]

    history = attention_cnn_model.fit(
        train_generator,
        validation_data=validation_generator,
        epochs=EPOCHS, # Assuming EPOCHS is defined
        verbose=1,
        callbacks=callbacks_list # Use the updated callbacks list
    )
    print("[INFO] Attention CNN model training on subset completed.")

    # Save the final model after training
    final_model_filepath = 'attention_cnn_model_final.keras'
    attention_cnn_model.save(final_model_filepath)
    print(f"[INFO] Final model saved to {final_model_filepath}")

else:
    print("[ERROR] Attention CNN model or data generators not available for training.")



"""## Display sample training images and labels

### Subtask:
Display a few sample images from the training subset along with their corresponding labels.

**Reasoning**:
Display a few sample images from the training subset along with their corresponding labels to visualize the data being used for training.
"""

import os
from sklearn.model_selection import train_test_split
import numpy as np # Import numpy
import matplotlib.pyplot as plt
import cv2

extracted_base_dir = '/content/extracted_augmented_data/bb_random_augmented_dataset'
images_dir = os.path.join(extracted_base_dir, 'random_augmented_images')
labels_dir = os.path.join(extracted_base_dir, 'random_augmented_labels')

all_image_files = []
all_labels = []

label_files = sorted([f for f in os.listdir(labels_dir) if f.endswith('.txt')])
for label_file_name in label_files:
    label_file_path = os.path.join(labels_dir, label_file_name)
    with open(label_file_path, 'r') as f:
        for line in f:
            parts = line.strip().split(',')
            if len(parts) == 2:
                image_filename = parts[0]
                label = int(parts[1])
                all_image_files.append(os.path.join(images_dir, image_filename))
                all_labels.append(label)

print(f"[INFO] Total number of image file paths loaded: {len(all_image_files)}")

# Select a random 20% subset of the total data
subset_size = int(len(all_image_files) * 0.2)
# Use train_test_split to get a random subset (by splitting with test_size=1-0.2=0.8)
subset_image_files, _, subset_labels, _ = train_test_split(
    all_image_files, all_labels, test_size=0.8, random_state=42, shuffle=True
)

print(f"[INFO] Selected 20% subset size: {len(subset_image_files)}")

# Now split this 20% subset into training and validation sets (e.g., 80/20 split of the subset)
train_image_files, val_image_files, train_labels, val_labels = train_test_split(
    subset_image_files, subset_labels, test_size=0.2, random_state=42, shuffle=True
)


print(f"[INFO] Training set size (from 20% subset): {len(train_image_files)}")
print(f"  Approximate percentage of total data: {len(train_image_files) / len(all_image_files) * 100:.2f}%")
print(f"[INFO] Validation set size (from 20% subset): {len(val_image_files)}")
print(f"  Approximate percentage of total data: {len(val_image_files) / len(all_image_files) * 100:.2f}%")

# Select a few sample images and labels from the training subset
num_samples_to_display = 5
sample_indices = np.random.choice(len(train_image_files), num_samples_to_display, replace=False)

plt.figure(figsize=(15, 5))

for i, idx in enumerate(sample_indices):
    image_file = train_image_files[idx]
    label_index = train_labels[idx] # Get the integer label index
    label = label_binarizer.classes_[label_index] # Convert index back to string label

    # Read and convert image
    image = cv2.imread(image_file)
    if image is not None:
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

        plt.subplot(1, num_samples_to_display, i + 1)
        plt.imshow(image)
        plt.title(label)
        plt.axis('off')
    else:
        print(f"Could not load image: {image_file}")


plt.tight_layout()
plt.show()

import random
import cv2
import numpy as np
import matplotlib.pyplot as plt

##### Upload test image in test_plan_image and do prediction

# Get the file path and label for the selected sample image
sample_image_file = '/content/test_plant_image.JPG'


print(f"[INFO] Selected sample image: {sample_image_file}")

# Load the sample image
sample_image = cv2.imread(sample_image_file)

if sample_image is not None:
    # Convert to RGB for displaying with matplotlib
    sample_image_rgb = cv2.cvtColor(sample_image, cv2.COLOR_BGR2RGB)

    # Display the original sample image
    plt.imshow(sample_image_rgb)
    plt.title(f"Sample Image: {sample_label}")
    plt.axis('off')
    plt.show()

    # Preprocess the image for the model (resize and normalize)
    # Assuming the model expects 64x64 images and float16 normalized data
    # You might need to adjust this based on your model's input requirements
    processed_sample_image = cv2.resize(sample_image, (width, height)) # Assuming width and height are defined
    processed_sample_image = processed_sample_image.astype(np.float16) / 255.0
    processed_sample_image = np.expand_dims(processed_sample_image, axis=0) # Add batch dimension


    print("[INFO] Sample image loaded and preprocessed for feature map visualization.")

else:
    print(f"[ERROR] Could not load sample image: {sample_image_file}")

import matplotlib.pyplot as plt
import cv2
import numpy as np

# Predict the disease for the processed sample image
# Assumes attention_cnn_model and processed_sample_image are defined in previous cells
if 'attention_cnn_model' in locals() and 'processed_sample_image' in locals():
    print("[INFO] Predicting disease for the sample image...")

    # Use the .predict() method of the attention_cnn_model with the processed_sample_image as input.
    predictions = attention_cnn_model.predict(processed_sample_image)

    # Get the predicted class index (the index with the highest probability)
    predicted_class_index = np.argmax(predictions[0])

    # Convert the predicted class index back to the disease label name
    # Assumes label_binarizer is defined and fitted
    if 'label_binarizer' in locals():
        predicted_label = label_binarizer.classes_[predicted_class_index]
        print(f"[INFO] Predicted disease: {predicted_label}")
    else:
        print("[ERROR] 'label_binarizer' not found. Cannot convert predicted index to label.")
        predicted_label = "Unknown"


    # Store the predicted label in a variable (e.g., predicted_disease)
    predicted_disease = predicted_label

    # Print a confirmation message and the predicted disease.
    print("[INFO] Prediction completed.")

    # --- Add code to display the image with true and predicted labels ---
    # Assumes sample_image_rgb and sample_label are defined in the preceding cell
    if 'sample_image_rgb' in locals() and 'sample_label' in locals() and sample_image_rgb is not None:
        plt.figure(figsize=(6, 6))
        plt.imshow(sample_image_rgb)
        plt.title(f"True Label: {sample_label}\nPredicted Label: {predicted_disease}")
        plt.axis('off') # Hide axes ticks and labels
        plt.show()
    else:
        print("[ERROR] 'sample_image_rgb' or 'sample_label' not found. Cannot display image with labels.")
    # --- End of added code ---

else:
    print("[ERROR] 'attention_cnn_model' or 'processed_sample_image' not found. Please run the preceding cells.")

"""# Predict model output by loading model from file and test on an image"""

import matplotlib.pyplot as plt
import cv2
import numpy as np
from tensorflow.keras.models import load_model # Import load_model

# Create a custom Keras Layer for reshaping (needs to be defined for loading)
class ReshapeLayer(Layer):
    def __init__(self, target_shape, **kwargs):
        super(ReshapeLayer, self).__init__(**kwargs)
        self.target_shape = target_shape

    def call(self, inputs):
        return tf.reshape(inputs, self.target_shape)

    def get_config(self):
        config = super(ReshapeLayer, self).get_config()
        config.update({
            'target_shape': self.target_shape,
        })
        return config

# Define the path to the saved model
model_path = 'attention_cnn_model_final.keras'

# 1. Load the trained model from the specified file path.
try:
    loaded_attention_model = load_model(model_path,custom_objects={'ReshapeLayer': ReshapeLayer})
    print(f"[INFO] Model loaded successfully from {model_path}.")
except Exception as e:
    print(f"[ERROR] Could not load model from {model_path}: {e}")
    loaded_attention_model = None # Set to None if loading fails


# Get the file path and label for the selected sample image
sample_image_file = '/content/test_plant_image.JPG'

print(f"[INFO] Selected sample image: {sample_image_file}")

# Load the sample image
sample_image = cv2.imread(sample_image_file)

if sample_image is not None:
    # Convert to RGB for displaying with matplotlib
    sample_image_rgb = cv2.cvtColor(sample_image, cv2.COLOR_BGR2RGB)

    # Preprocess the image for the model (resize and normalize)
    # Assuming the model expects 64x64 images and float16 normalized data
    # You might need to adjust this based on your model's input requirements
    processed_sample_image = cv2.resize(sample_image, (width, height)) # Assuming width and height are defined
    processed_sample_image = processed_sample_image.astype(np.float16) / 255.0
    processed_sample_image = np.expand_dims(processed_sample_image, axis=0) # Add batch dimension


    print("[INFO] Sample image loaded and preprocessed for feature map visualization.")

else:
    print(f"[ERROR] Could not load sample image: {sample_image_file}")

# Predict the disease for the processed sample image
# Assumes processed_sample_image is defined in previous cells
if loaded_attention_model is not None and 'processed_sample_image' in locals():
    print("[INFO] Predicting disease for the sample image using the loaded model...")

    # Use the .predict() method of the loaded_attention_model with the processed_sample_image as input.
    predictions = loaded_attention_model.predict(processed_sample_image)

    # Get the predicted class index (the index with the highest probability)
    predicted_class_index = np.argmax(predictions[0])

    # Convert the predicted class index back to the disease label name
    # Assumes label_binarizer is defined and fitted
    if 'label_binarizer' in locals():
        predicted_label = label_binarizer.classes_[predicted_class_index]
        print(f"[INFO] Predicted disease: {predicted_label}")
    else:
        print("[ERROR] 'label_binarizer' not found. Cannot convert predicted index to label.")
        predicted_label = "Unknown"


    # Store the predicted label in a variable (e.g., predicted_disease)
    predicted_disease = predicted_label

    # Print a confirmation message and the predicted disease.
    print("[INFO] Prediction completed.")

    # --- Add code to display the image with true and predicted labels ---
    # Assumes sample_image_rgb and sample_label are defined in the preceding cell
    if 'sample_image_rgb' in locals() and 'sample_label' in locals() and sample_image_rgb is not None:
        plt.figure(figsize=(6, 6))
        plt.imshow(sample_image_rgb)
        plt.title(f"True Label: {sample_label}\nPredicted Label: {predicted_disease}")
        plt.axis('off') # Hide axes ticks and labels
        plt.show()
    else:
        print("[ERROR] 'sample_image_rgb' or 'sample_label' not found. Cannot display image with labels.")
    # --- End of added code ---

else:
    print("[ERROR] Loaded model or 'processed_sample_image' not available for prediction.")

"""## Prepare the model for feature map visualization using the attention model

### Subtask:
Create a new Keras model that outputs the activations of intermediate layers of the `attention_cnn_model`.

**Reasoning**:
Define the input layer for the feature map model and iterate through the layers of the attention_cnn_model to collect their outputs.
"""

import tensorflow as tf
from keras.models import Model
from keras.layers import Input

# Get the input layer of the original attention_cnn_model
original_input = attention_cnn_model.input

# Get the outputs of all layers in the original attention_cnn_model
layer_outputs = [layer.output for layer in attention_cnn_model.layers]

# Create a new model that takes the original model's input and outputs the outputs of all layers
feature_map_model = Model(inputs=original_input, outputs=layer_outputs)

print("[INFO] Feature map model created successfully.")
# Note: Model summary might be very long as it includes all intermediate outputs
# feature_map_model.summary()

import random
import cv2
import numpy as np
import matplotlib.pyplot as plt

# Select a random index from the training image files
sample_image_index = random.randint(0, len(train_image_files) - 1)

# Get the file path and label for the selected sample image
sample_image_file = train_image_files[sample_image_index]
sample_label_index = train_labels[sample_image_index]
sample_label = label_binarizer.classes_[sample_label_index] # Convert index back to string label


print(f"[INFO] Selected sample image: {sample_image_file}")
print(f"[INFO] Corresponding label: {sample_label}")

# Load the sample image
sample_image = cv2.imread(sample_image_file)

if sample_image is not None:
    # Convert to RGB for displaying with matplotlib
    sample_image_rgb = cv2.cvtColor(sample_image, cv2.COLOR_BGR2RGB)

    # Display the original sample image
    plt.imshow(sample_image_rgb)
    plt.title(f"Sample Image: {sample_label}")
    plt.axis('off')
    plt.show()

    # Preprocess the image for the model (resize and normalize)
    # Assuming the model expects 64x64 images and float16 normalized data
    # You might need to adjust this based on your model's input requirements
    processed_sample_image = cv2.resize(sample_image, (width, height)) # Assuming width and height are defined
    processed_sample_image = processed_sample_image.astype(np.float16) / 255.0
    processed_sample_image = np.expand_dims(processed_sample_image, axis=0) # Add batch dimension


    print("[INFO] Sample image loaded and preprocessed for feature map visualization.")

else:
    print(f"[ERROR] Could not load sample image: {sample_image_file}")



# Predict the feature maps for the example image
# Assumes feature_map_model and processed_sample_image are defined in previous cells
if 'feature_map_model' in locals() and 'processed_sample_image' in locals():
    print("[INFO] Predicting feature maps...")
    feature_maps = feature_map_model.predict(processed_sample_image)
    print("[INFO] Feature map prediction completed.")

    # Visualize feature maps for a few layers
    # Assumes attention_cnn_model is defined
    layer_names = [layer.name for layer in attention_cnn_model.layers]

    images_per_row = 16 # Number of feature maps to display per row

    print("[INFO] Visualizing feature maps...")
    for layer_name, feature_map in zip(layer_names, feature_maps):
        # Do not plot the feature maps of layers that don't have a 4D output (e.g., Flatten, Dense)
        # Also handle the attention layer output which is 3D (batch, sequence_length, features)
        if feature_map.ndim not in [3, 4]:
            continue

        # Handle 4D convolutional/pooling layer outputs
        if feature_map.ndim == 4:
            # Number of features in the feature map
            n_features = feature_map.shape[-1]
            # The feature map has shape (1, size, size, n_features).
            size = feature_map.shape[1]

            # We will tile the activation channels in this matrix
            display_grid = np.zeros((size, images_per_row * size))

            # We'll tile each filter into this grid
            for col in range(min(n_features, images_per_row)): # Display up to images_per_row feature maps
                # Get a feature map from the current layer
                channel_image = feature_map[0, :, :, col]

                # Post-process the feature map to make it visually palatable
                # Avoid division by zero if std is zero
                if channel_image.std() != 0:
                    channel_image = (channel_image - channel_image.mean()) / channel_image.std()
                channel_image = channel_image * 64 + 128
                channel_image = np.clip(channel_image, 0, 255).astype('uint8')

                # Displays the grid
                display_grid[:, col * size : (col + 1) * size] = channel_image

        # Handle 3D attention layer output (batch, sequence_length, features)
        elif feature_map.ndim == 3 and layer_name == 'attention':
             # For visualization, treat sequence_length as a spatial dimension and features as channels
            sequence_length = feature_map.shape[1]
            n_features = feature_map.shape[2]

            # Determine a reasonable grid size for visualization
            # For simplicity, let's try to arrange the sequence elements in a near-square grid
            grid_size = int(np.ceil(np.sqrt(sequence_length)))
            display_grid_size = grid_size * images_per_row # Display a grid of grid_size x grid_size blocks

            display_grid = np.zeros((grid_size * grid_size, images_per_row * n_features)) # Adjust grid size for features

            # Iterate through sequence elements and display a few feature dimensions
            for i in range(min(sequence_length, grid_size * grid_size)):
                # Get the feature vector for the current sequence element
                feature_vector = feature_map[0, i, :]

                # Reshape the feature vector to a small spatial representation if needed,
                # or just display a portion of the features.
                # For now, let's display a portion of the features for a few sequence elements.

                # Simple approach: Tile a few feature dimensions
                for col in range(min(n_features, images_per_row)):
                     channel_value = feature_vector[col]

                     # Create a small block to represent this feature value
                     block_size = grid_size # Use grid_size for block size for visualization

                     # Post-process the feature value to make it visually palatable
                     # This is a simplified approach compared to processing a whole channel image
                     block_image = np.full((block_size, block_size), channel_value)

                     # Simple normalization and scaling
                     block_image = (block_image - np.mean(block_image)) / (np.std(block_image) if np.std(block_image) != 0 else 1e-5)
                     block_image = block_image * 64 + 128
                     block_image = np.clip(block_image, 0, 255).astype('uint8')


                     # Calculate position in the display grid
                     row_idx = i // grid_size
                     col_idx = i % grid_size

                     # Place the block in the display grid
                     # This tiling logic needs adjustment based on how we want to arrange the sequence elements
                     # Let's simplify and just display the feature vectors as rows of pixels

            # A more straightforward way to visualize the attention layer output as a "feature map"
            # is to reshape the sequence_length dimension back to the original spatial dimensions (e.g., 5x5)
            # if the original spatial dimensions are known and sequence_length is size*size.
            # Assuming the flattened spatial size before attention was 5x5 (from MaxPooling2D_2 output size)
            original_spatial_size = int(np.sqrt(sequence_length)) # Assuming sequence_length is a perfect square of the original spatial size

            if original_spatial_size * original_spatial_size == sequence_length:
                # Reshape to (batch, original_spatial_size, original_spatial_size, features)
                reshaped_attention_output = np.reshape(feature_map, (-1, original_spatial_size, original_spatial_size, n_features))

                # Now visualize this reshaped output like a normal 4D feature map
                size = original_spatial_size
                display_grid = np.zeros((size, images_per_row * size))

                for col in range(min(n_features, images_per_row)):
                     channel_image = reshaped_attention_output[0, :, :, col]

                     # Post-process
                     if channel_image.std() != 0:
                         channel_image = (channel_image - channel_image.mean()) / channel_image.std()
                     channel_image = channel_image * 64 + 128
                     channel_image = np.clip(channel_image, 0, 255).astype('uint8')

                     display_grid[:, col * size : (col + 1) * size] = channel_image

            else:
                 print(f"Warning: Could not reshape attention output of shape {feature_map.shape} to a square spatial grid for visualization.")
                 continue # Skip visualization for this layer if reshaping is not possible


        # Display the grid if it was created
        if 'display_grid' in locals() and display_grid.shape[0] > 0 and display_grid.shape[1] > 0:
             scale = 1. / size
             plt.figure(figsize=(scale * display_grid.shape[1],
                                scale * display_grid.shape[0]))
             plt.title(layer_name)
             plt.grid(False)
             plt.imshow(display_grid, aspect='auto', cmap='viridis')
             del display_grid # Clear display_grid for the next layer


    plt.show()

else:
    print("[ERROR] 'feature_map_model' or 'processed_sample_image' not found. Please run the preceding cells.")



"""## Modify the model to output attention weights

### Subtask:
Create a new Keras model or modify the existing `attention_cnn_model` to output the attention weights from the `Attention` layer in addition to the final output.

**Reasoning**:
Create a new Keras model that outputs the attention weights from the Attention layer in addition to the final output.
"""

from keras.models import Model

# Identify the Attention layer within the attention_cnn_model
attention_layer = None
for layer in attention_cnn_model.layers:
    if isinstance(layer, tf.keras.layers.Attention):
        attention_layer = layer
        break

if attention_layer:
    # Create a new Keras Model with multiple outputs
    attention_model_with_weights = Model(
        inputs=attention_cnn_model.input,
        outputs=[attention_cnn_model.output, attention_layer.output]
    )
    print("[INFO] Attention model with weights output created successfully.")
    # You can optionally print the summary to verify the outputs
    # attention_model_with_weights.summary()
else:
    print("[ERROR] Attention layer not found in the attention_cnn_model.")
    attention_model_with_weights = None

"""**Reasoning**:
Use the modified model to predict the attention weights for the selected sample image.


"""

# 1. Check if the attention_model_with_weights and processed_sample_image variables exist.
if 'attention_model_with_weights' in locals() and 'processed_sample_image' in locals():
    print("[INFO] Required variables found. Predicting attention weights...")

    # 2. Use the .predict() method of attention_model_with_weights with processed_sample_image as input.
    # Since the model is loaded from a .keras file and potentially has multiple outputs
    # (as seen in previous feature map visualization), the prediction will return a list.
    # The first element of this list should be the final classification output (class probabilities).
    predictions = attention_model_with_weights.predict(processed_sample_image)

    # Check if the prediction output is a list and get the first element (final output)
    if isinstance(predictions, list):
        prediction = predictions[0] # Assuming the first element is the class probabilities
    else:
        prediction = predictions # If it's not a list, assume it's the final output directly


    # 2. Store the prediction results (class probabilities) in a variable (e.g., prediction).
    # This is done in the previous step.

    # 3. Print a confirmation message indicating that the prediction was made and display the shape of the prediction array.
    print("[INFO] Prediction made successfully.")
    print(f"Shape of the prediction array (class probabilities): {prediction.shape}")

else:
    print("[ERROR] 'loaded_attention_model' or 'processed_new_image' not found. Please run the preceding cells.")

"""## Process attention weights

### Subtask:
Reshape and potentially aggregate the attention weights to correspond to the spatial dimensions of the original image.

"""

# Use the loaded model to predict the class probabilities for the preprocessed image.
# Assumes loaded_attention_model and processed_new_image are defined in previous cells.
if 'attention_model_with_weights' in locals() and 'processed_sample_image' in locals():
    print("[INFO] Using the loaded model to predict the class probabilities...")

    # Use the .predict() method of the loaded_attention_model with the processed_new_image as input.
    # The prediction will return a list of outputs; the second element in this list will be the attention weights. Store these attention weights in a variable.
    predictions = attention_model_with_weights.predict(processed_sample_image)

    # Check if the prediction output is a list and get the second element (attention output)
    if isinstance(predictions, list) and len(predictions) > 1:
        sample_attention_weights = predictions[1] # Assuming the second element is the attention weights
        print("[INFO] Attention weights obtained successfully.")
        print(f"Shape of attention weights: {sample_attention_weights.shape}")
    else:
        print("[ERROR] Could not obtain attention weights from the model prediction.")
        sample_attention_weights = None


else:
    print("[ERROR] 'attention_model_with_weights' or 'processed_sample_image' not found. Please run the preceding cells.")

"""**Reasoning**:
Now that `sample_attention_weights` has been successfully obtained, I can proceed with reshaping and aggregating the attention weights to correspond to the spatial dimensions of the original image as per the subtask instructions.


"""

import numpy as np
import math

# 1. Check if the sample_attention_weights variable exists.
if 'sample_attention_weights' not in locals():
    print("[ERROR] 'sample_attention_weights' not found. Please run the preceding cells.")
else:
    print("[INFO] 'sample_attention_weights' found. Proceeding with reshaping and aggregation.")

    # 2. Get the original spatial size of the feature maps before the attention layer.
    # The shape of the input to the Attention layer was (batch_size, sequence_length, features)
    # In our case, the sequence_length is sample_attention_weights.shape[1]
    sequence_length = sample_attention_weights.shape[1]
    original_spatial_size = int(np.sqrt(sequence_length))

    # 3. Check if original_spatial_size squared is equal to sequence_length.
    if original_spatial_size * original_spatial_size != sequence_length:
        print(f"[WARNING] Sequence length ({sequence_length}) is not a perfect square of the calculated spatial size ({original_spatial_size}). Reshaping to spatial dimensions may not be accurate.")
        # Depending on the task requirements, you might stop here or proceed with a different visualization approach
        # For this subtask, we will proceed as requested, but acknowledge the potential inaccuracy.

    # 4. Reshape sample_attention_weights from (batch_size, sequence_length, features) to (batch_size, original_spatial_size, original_spatial_size, features).
    # We have a batch size of 1 for a single sample.
    batch_size = sample_attention_weights.shape[0]
    features = sample_attention_weights.shape[2]
    spatial_attention_weights = np.reshape(sample_attention_weights, (batch_size, original_spatial_size, original_spatial_size, features))
    print(f"[INFO] Reshaped attention weights to spatial dimensions. New shape: {spatial_attention_weights.shape}")


    # 5. Aggregate the attention weights across the feature dimension.
    # Using np.mean() to get a single attention value per spatial location.
    aggregated_attention_map = np.mean(spatial_attention_weights, axis=-1)
    print(f"[INFO] Aggregated attention weights across features. New shape: {aggregated_attention_map.shape}")

    # 6. Remove the batch dimension from aggregated_attention_map.
    # Since batch_size is 1, we can simply squeeze the first dimension.
    final_attention_map = np.squeeze(aggregated_attention_map, axis=0)

    # 7. Print the shape of final_attention_map.
    print(f"[INFO] Final attention map created. Shape: {final_attention_map.shape}")

"""## Visualize the attention map

### Subtask:
Display the processed attention map as a heatmap.

**Reasoning**:
Display the processed attention map as a heatmap using matplotlib.
"""

import matplotlib.pyplot as plt

# Check if the final_attention_map variable exists
if 'final_attention_map' not in locals():
    print("[ERROR] 'final_attention_map' not found. Please run the preceding cells.")
else:
    print("[INFO] 'final_attention_map' found. Visualizing attention map.")

    # Visualize the attention map
    plt.figure(figsize=(6, 6))
    plt.imshow(final_attention_map, cmap='viridis') # Using 'viridis' cmap as in previous visualizations
    plt.title('Aggregated Attention Map')
    plt.colorbar(label='Attention Weight (Mean)')
    plt.axis('off') # Hide axes ticks and labels
    plt.show()

"""**Reasoning**:
Overlay the attention map on the original image for better context.


"""

import cv2
import numpy as np

# Optional: Overlay the attention map on the original image for better context
# This requires resizing the attention map to the original image dimensions.
# Assumes sample_image_rgb is available from previous steps

if 'sample_image_rgb' in locals() and sample_image_rgb is not None:
    print("[INFO] sample_image_rgb found. Attempting to overlay attention map.")
    # Resize the attention map to the original image dimensions
    # Use interpolation to smooth the resized map
    resized_attention_map = cv2.resize(final_attention_map, (sample_image_rgb.shape[1], sample_image_rgb.shape[0]), interpolation=cv2.INTER_CUBIC)

    # Normalize the resized attention map to [0, 1] for alpha blending
    resized_attention_map_normalized = (resized_attention_map - np.min(resized_attention_map)) / (np.max(resized_attention_map) - np.min(resized_attention_map))

    # Create a heatmap from the normalized attention map
    heatmap = cv2.applyColorMap(np.uint8(255 * resized_attention_map_normalized), cv2.COLORMAP_VIRIDIS)
    heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB) # Convert BGR to RGB


    # Blend the heatmap with the original image
    # Use the normalized attention map as the alpha channel for blending
    # Ensure both images are of the same data type and shape (except for channels for original image)
    # Convert original image and heatmap to float for blending
    original_image_float = sample_image_rgb.astype(np.float32) / 255.0
    heatmap_float = heatmap.astype(np.float32) / 255.0

    # Expand the normalized attention map to 3 channels to match image shape
    alpha = np.expand_dims(resized_attention_map_normalized, axis=-1)

    # Perform alpha blending: output = (1 - alpha) * original + alpha * heatmap
    overlay_image = (1 - alpha) * original_image_float + alpha * heatmap_float
    overlay_image = np.clip(overlay_image, 0, 1) # Clip values to be within [0, 1]


    # Display the overlayed image
    plt.figure(figsize=(8, 8))
    plt.imshow(overlay_image)
    plt.title(f'Attention Map Overlay on Original Image ({sample_label})')
    plt.axis('off')
    plt.show()

else:
    print("[INFO] sample_image_rgb not found. Skipping attention map overlay.")

"""# C. ResNet Model for entire Dataset"""

from tensorflow.keras.applications import ResNet50

# Define the input shape
input_shape = (height, width, depth) # Assuming height, width, and depth are defined

# Load the pre-trained ResNet50 model, excluding the top classification layer
resnet_base = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)

print("[INFO] Pre-trained ResNet50 base model loaded successfully.")

"""## Modify the resnet base model for the custom dataset

### Subtask:
Add new layers on top of the ResNet base model to adapt it for your plant disease classification task (e.g., flatten the output and add dense layers with the correct number of output classes).

**Reasoning**:
Add flatten and dense layers on top of the ResNet base model to adapt it for the plant disease classification task.
"""

from keras.layers import Flatten, Dense
from keras.models import Model
from keras.layers import Input

# Get the output of the resnet_base model
input_tensor = Input(shape=input_shape)
resnet_output = resnet_base(input_tensor)

# Flatten the output of the resnet_base model
flatten_layer = Flatten()(resnet_output)

# Add a Dense layer with a suitable number of units and a ReLU activation function
dense_layer_1 = Dense(1024, activation='relu')(flatten_layer)

# Add another Dense layer with n_classes units and a softmax activation function
output_layer = Dense(n_classes, activation='softmax')(dense_layer_1)

# Create a new Keras Model
resnet_model = Model(inputs=input_tensor, outputs=output_layer)

print("[INFO] ResNet model with added layers created successfully.")

# Optionally, print the summary of the new model
resnet_model.summary()

"""## Compile the modified resnet model

### Subtask:
Compile the new ResNet-based model with an appropriate optimizer, loss function, and metrics.

**Reasoning**:
Compile the ResNet-based model with the Adam optimizer and binary crossentropy loss.
"""

from tensorflow.keras.optimizers import Adam

# Compile the resnet_model
opt = Adam(learning_rate=INIT_LR) # Decay is ignored in newer Keras versions
# Change loss function to categorical_crossentropy for multi-class classification
resnet_model.compile(loss="categorical_crossentropy", optimizer=opt, metrics=["accuracy"])

print("[INFO] ResNet model compiled successfully with categorical_crossentropy.")

import tensorflow as tf
import numpy as np
import os
import cv2
import math
from sklearn.model_selection import train_test_split # Import train_test_split

class SplitAugmentedDataGenerator(tf.keras.utils.Sequence):
    """
    Custom data generator that loads augmented images and labels in batches
    and performs train/validation split internally based on file paths.
    """
    def __init__(self, extracted_base_dir, label_binarizer, batch_size=32, shuffle=True, validation_split=0.2, is_validation=False):
        self.extracted_base_dir = extracted_base_dir
        self.label_binarizer = label_binarizer
        self.batch_size = batch_size
        self.shuffle = shuffle
        self.validation_split = validation_split
        self.is_validation = is_validation

        self.extracted_images_dir = os.path.join(self.extracted_base_dir, 'random_augmented_images')
        self.extracted_labels_dir = os.path.join(self.extracted_base_dir, 'random_augmented_labels')

        # Load all image file paths and corresponding labels from the label files
        all_image_files = []
        all_labels = []

        label_files = sorted([f for f in os.listdir(self.extracted_labels_dir) if f.endswith('.txt')])
        for label_file_name in label_files:
            label_file_path = os.path.join(self.extracted_labels_dir, label_file_name)
            with open(label_file_path, 'r') as f:
                for line in f:
                    parts = line.strip().split(',')
                    if len(parts) == 2:
                        image_filename = parts[0]
                        label = int(parts[1])
                        all_image_files.append(os.path.join(self.extracted_images_dir, image_filename))
                        all_labels.append(label)

        # Perform the train/validation split on the file paths and labels
        # We use a fixed random state to ensure the split is the same for both generators
        if not is_validation:
            self.image_files, _, self.labels, _ = train_test_split(
                all_image_files, all_labels, test_size=validation_split, random_state=42, shuffle=self.shuffle
            )
        else:
            _, self.image_files, _, self.labels = train_test_split(
                all_image_files, all_labels, test_size=validation_split, random_state=42, shuffle=self.shuffle
            )


        self.on_epoch_end() # Initial shuffling if needed

    def __len__(self):
        """Denotes the number of batches per epoch."""
        return math.ceil(len(self.image_files) / self.batch_size)

    def __getitem__(self, index):
        """Generate one batch of data."""
        # Generate indexes of the batch
        indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]

        # Find list of IDs for the batch
        batch_image_files = [self.image_files[k] for k in indexes]
        batch_labels = [self.labels[k] for k in indexes]

        # Generate data
        X, y = self.__data_generation(batch_image_files, batch_labels)

        return X, y

    def on_epoch_end(self):
        """Updates indexes after each epoch."""
        self.indexes = np.arange(len(self.image_files))
        if self.shuffle == True:
            np.random.shuffle(self.indexes)

    def __data_generation(self, batch_image_files, batch_labels):
        """Generates data containing batch_size samples."""
        # Initialization
        # Assuming all images have the same dimensions (e.g., 64x64x3)
        img_height, img_width, img_depth = 64, 64, 3 # Use your defined image shape
        X = np.empty((len(batch_image_files), img_height, img_width, img_depth), dtype=np.float16)
        y = np.empty((len(batch_image_files), len(self.label_binarizer.classes_)), dtype=np.float32) # Use float32 for one-hot encoding

        # Generate data
        for i, (image_file, label) in enumerate(zip(batch_image_files, batch_labels)):
            # Load image
            image = cv2.imread(image_file)
            if image is not None:
                # Convert image to float and normalize
                image = image.astype(np.float16) / 255.0
                X[i,] = image

                # Convert label to one-hot encoding
                #y[i,] = self.label_binarizer.transform([label])[0]
                y[i,] = self.label_binarizer.transform([label_binarizer.classes_[label]])[0]

            else:
                print(f"Warning: Could not load image {image_file} in data generator. Filling with zeros.")
                # Handle missing images - fill with zeros or a placeholder
                X[i,] = np.zeros((img_height, img_width, img_depth), dtype=np.float16)
                y[i,] = np.zeros(len(self.label_binarizer.classes_), dtype=np.float32)


        return X, y

# --- Example Usage ---
# Assuming you have extracted the zip file to '/content/extracted_augmented_data'
# and have the label_binarizer object available from previous steps

# Define the base directory where the zip extracted its contents
extracted_base_dir = '/content/extracted_augmented_data/bb_random_augmented_dataset' # Use the directory containing image and label folders

# Make sure 'label_binarizer' and 'BS' (batch size) are defined in previous cells

if 'label_binarizer' in locals() and 'BS' in locals():
    # Create the training data generator
    train_generator = SplitAugmentedDataGenerator(
        extracted_base_dir=extracted_base_dir,
        label_binarizer=label_binarizer,
        batch_size=BS,
        shuffle=True,
        validation_split=0.2, # Define your desired validation split
        is_validation=False
    )
    print("[INFO] Training data generator created.")
    print(f"Number of training batches per epoch: {len(train_generator)}")
    print(f"Total number of training images: {len(train_generator.image_files)}")


    # Create the validation data generator
    validation_generator = SplitAugmentedDataGenerator(
        extracted_base_dir=extracted_base_dir,
        label_binarizer=label_binarizer,
        batch_size=BS,
        shuffle=False, # No need to shuffle validation data
        validation_split=0.2, # Use the same validation split
        is_validation=True # Set to True for the validation generator
    )
    print("[INFO] Validation data generator created.")
    print(f"Number of validation batches per epoch: {len(validation_generator)}")
    print(f"Total number of validation images: {len(validation_generator.image_files)}")

else:
    print("[ERROR] 'label_binarizer' or 'BS' not found. Please run the cells that define them.")

# Define the accuracy callback for early stopping
accuracy_callback = AccuracyThresholdCallback(acc_threshold=0.9, val_acc_threshold=0.9)
print("[INFO] AccuracyThresholdCallback created.")

# Define ModelCheckpoint callback
checkpoint_filepath = '/tmp/checkpoint/best_model.h5' # Define checkpoint file path
model_checkpoint_callback = ModelCheckpoint(
    filepath=checkpoint_filepath,
    save_best_only=True, # Save only the best model based on the monitored metric
    monitor='val_accuracy', # Monitor validation accuracy
    mode='max', # Save when validation accuracy is maximum
    verbose=1
)
print(f"[INFO] ModelCheckpoint callback created. Best model will be saved to {checkpoint_filepath}")


# Train the attention CNN model using the subset data generators
# Assumes attention_cnn_model is defined and compiled
if 'resnet_model' in locals() and train_generator is not None and validation_generator is not None:
    print("[INFO] Training  Resnet CNN model on the entire subset...")

    # Add the ModelCheckpoint callback to the callbacks list
    callbacks_list = [callback for callback in [accuracy_callback, model_checkpoint_callback] if callback is not None]

    history = resnet_model.fit(
        train_generator,
        validation_data=validation_generator,
        epochs=EPOCHS, # Assuming EPOCHS is defined
        verbose=1,
        callbacks=callbacks_list # Use the updated callbacks list
    )
    print("[INFO] Resnet CNN model training on subset completed.")

    # Save the final model after training
    final_model_filepath = 'resnet_cnn_model_final.keras'
    resnet_model.save(final_model_filepath)
    print(f"[INFO] Final model saved to {final_model_filepath}")

else:
    print("[ERROR] Resnet CNN model or data generators not available for training.")



"""# D. Voting Classifier multiple models

Try 5 smaller cnn models to avoid the latency issues but for that we need to divide dataset into subsets of 20%
"""



"""# Task
Change the model training code to make sure it stops when accuracy and validation accuracy is more than 90%.

## Agent

### Subtask:
Prepare the dataset for splitting

**Reasoning**:
Verify the existence of the extracted directories and their contents to confirm successful extraction and preparation for dataset splitting.
"""

import os

extracted_base_dir = '/content/extracted_augmented_data/random_augmented_dataset'
images_dir = os.path.join(extracted_base_dir, 'random_augmented_images')
labels_dir = os.path.join(extracted_base_dir, 'random_augmented_labels')

# Check if the base directory and subdirectories exist
if os.path.exists(extracted_base_dir):
    print(f"Base extracted directory found: {extracted_base_dir}")
    if os.path.exists(images_dir):
        print(f"Images directory found: {images_dir}")
        # Check for image files
        image_files = [f for f in os.listdir(images_dir) if f.endswith('.jpg') or f.endswith('.JPG')]
        if image_files:
            print(f"Found {len(image_files)} image files in {images_dir}. Displaying first 5:")
            for i, f in enumerate(image_files[:5]):
                print(f"- {f}")
        else:
            print(f"No image files found in {images_dir}.")
    else:
        print(f"Images directory not found: {images_dir}")

    if os.path.exists(labels_dir):
        print(f"Labels directory found: {labels_dir}")
        # Check for label files
        label_files = [f for f in os.listdir(labels_dir) if f.endswith('.txt')]
        if label_files:
            print(f"Found {len(label_files)} label files in {labels_dir}. Displaying first 5:")
            for i, f in enumerate(label_files[:5]):
                print(f"- {f}")
            # Optionally, read a few lines from a label file to verify content format
            if label_files:
                first_label_file = os.path.join(labels_dir, label_files[0])
                print(f"\nReading first 5 lines of {label_files[0]}:")
                with open(first_label_file, 'r') as f:
                    for j, line in enumerate(f):
                        if j < 5:
                            print(line.strip())
                        else:
                            break
        else:
            print(f"No label files found in {labels_dir}.")
    else:
        print(f"Labels directory not found: {labels_dir}")
else:
    print(f"Base extracted directory not found: {extracted_base_dir}")

"""## Create data subsets

### Subtask:
Randomly split the 300K dataset into 5 subsets of approximately 60K images and corresponding labels.

**Reasoning**:
Load the image file paths and labels from the extracted directories, combine them, shuffle the combined list, and then split it into 5 subsets.
"""

import os
import random

extracted_base_dir = '/content/extracted_augmented_data/random_augmented_dataset'
images_dir = os.path.join(extracted_base_dir, 'random_augmented_images')
labels_dir = os.path.join(extracted_base_dir, 'random_augmented_labels')

all_image_files = []
all_labels = []

label_files = sorted([f for f in os.listdir(labels_dir) if f.endswith('.txt')])
for label_file_name in label_files:
    label_file_path = os.path.join(labels_dir, label_file_name)
    with open(label_file_path, 'r') as f:
        for line in f:
            parts = line.strip().split(',')
            if len(parts) == 2:
                image_filename = parts[0]
                label = int(parts[1])
                all_image_files.append(os.path.join(images_dir, image_filename))
                all_labels.append(label)

combined_data = list(zip(all_image_files, all_labels))
random.shuffle(combined_data)

num_subsets = 5
subset_size = len(combined_data) // num_subsets
subsets = [combined_data[i * subset_size:(i + 1) * subset_size] for i in range(num_subsets)]

# Handle any remaining elements
if len(combined_data) % num_subsets != 0:
    subsets[-1].extend(combined_data[num_subsets * subset_size:])

split_datasets = []
for subset in subsets:
    subset_image_files, subset_labels = zip(*subset)
    split_datasets.append((list(subset_image_files), list(subset_labels)))

print(f"[INFO] Split the dataset into {len(split_datasets)} subsets.")
for i, (image_files, labels) in enumerate(split_datasets):
    print(f"  Subset {i+1}: {len(image_files)} images, {len(labels)} labels")

"""## Define the base cnn model architecture

### Subtask:
Create a function or block of code to define the CNN model architecture (the same one you're currently using).

**Reasoning**:
Define a function to create the CNN model architecture.
"""

from keras.models import Sequential
from keras.layers import BatchNormalization, Conv2D, MaxPooling2D, Activation, Flatten, Dropout, Dense
from keras import backend as K
from tensorflow.keras.optimizers import Adam

def build_cnn_model(input_shape, n_classes, init_lr=1e-3, epochs=25):
    """Builds a CNN model with the specified architecture."""
    model = Sequential()
    chanDim = -1
    if K.image_data_format() == "channels_first":
        chanDim = 1

    model.add(Conv2D(32, (3, 3), padding="same", input_shape=input_shape))
    model.add(Activation("relu"))
    model.add(BatchNormalization(axis=chanDim))
    model.add(MaxPooling2D(pool_size=(3, 3)))
    model.add(Dropout(0.25))

    model.add(Conv2D(64, (3, 3), padding="same"))
    model.add(Activation("relu"))
    model.add(BatchNormalization(axis=chanDim))
    model.add(Conv2D(64, (3, 3), padding="same"))
    model.add(Activation("relu"))
    model.add(BatchNormalization(axis=chanDim))
    model.add(MaxPooling2D(pool_size=(2, 2)))
    model.add(Dropout(0.25))

    model.add(Conv2D(128, (3, 3), padding="same"))
    model.add(Activation("relu"))
    model.add(BatchNormalization(axis=chanDim))
    model.add(Conv2D(128, (3, 3), padding="same"))
    model.add(Activation("relu"))
    model.add(BatchNormalization(axis=chanDim))
    model.add(MaxPooling2D(pool_size=(2, 2)))
    model.add(Dropout(0.25))

    model.add(Flatten())
    model.add(Dense(1024))
    model.add(Activation("relu"))
    model.add(BatchNormalization())
    model.add(Dropout(0.5))

    model.add(Dense(n_classes))
    model.add(Activation("softmax"))

    # Compile the model
    opt = Adam(learning_rate=init_lr, decay=init_lr / epochs)
    model.compile(loss="binary_crossentropy", optimizer=opt, metrics=["accuracy"])

    return model

# Example usage (optional, can be removed later if not needed for this subtask)
# input_shape = (height, width, depth) # Assuming height, width, depth are defined
# n_classes = len(label_binarizer.classes_) # Assuming label_binarizer is defined
# cnn_model = build_cnn_model(input_shape, n_classes, INIT_LR, EPOCHS)
# cnn_model.summary()

"""## Create and train multiple cnn models

### Subtask:
Instantiate 5 instances of the CNN model and train each one on a different 60K data subset.

**Reasoning**:
Iterate through the split datasets, create data generators for each subset, build a CNN model for each, train the model, and store the trained models.
"""

import tensorflow as tf
import numpy as np
import os
import cv2
import math
from sklearn.model_selection import train_test_split # Import train_test_split
from keras.models import Sequential
from keras.layers import BatchNormalization, Conv2D, MaxPooling2D, Activation, Flatten, Dropout, Dense
from keras import backend as K
from tensorflow.keras.optimizers import Adam
from keras.callbacks import Callback # Import Callback class

# Define the AccuracyThresholdCallback class
class AccuracyThresholdCallback(Callback):
    def __init__(self, acc_threshold=0.9, val_acc_threshold=0.9):
        super(AccuracyThresholdCallback, self).__init__()
        self.acc_threshold = acc_threshold
        self.val_acc_threshold = val_acc_threshold

    def on_epoch_end(self, epoch, logs=None):
        if logs is None:
            logs = {}
        acc = logs.get('accuracy')
        val_acc = logs.get('val_accuracy')
        if acc is not None and val_acc is not None:
            if acc >= self.acc_threshold and val_acc >= self.val_threshold:
                print(f"\nStopping training as accuracy ({acc:.4f}) and validation accuracy ({val_acc:.4f}) reached the threshold.")
                self.model.stop_training = True

# Define the build_cnn_model function
def build_cnn_model(input_shape, n_classes, init_lr=1e-3, epochs=25):
    """Builds a CNN model with the specified architecture."""
    model = Sequential()
    chanDim = -1
    if K.image_data_format() == "channels_first":
        chanDim = 1

    model.add(Conv2D(32, (3, 3), padding="same", input_shape=input_shape))
    model.add(Activation("relu"))
    model.add(BatchNormalization(axis=chanDim))
    model.add(MaxPooling2D(pool_size=(3, 3)))
    model.add(Dropout(0.25))

    model.add(Conv2D(64, (3, 3), padding="same"))
    model.add(Activation("relu"))
    model.add(BatchNormalization(axis=chanDim))
    model.add(Conv2D(64, (3, 3), padding="same"))
    model.add(Activation("relu"))
    model.add(BatchNormalization(axis=chanDim))
    model.add(MaxPooling2D(pool_size=(2, 2)))
    model.add(Dropout(0.25))

    model.add(Conv2D(128, (3, 3), padding="same"))
    model.add(Activation("relu"))
    model.add(BatchNormalization(axis=chanDim))
    model.add(Conv2D(128, (3, 3), padding="same"))
    model.add(Activation("relu"))
    model.add(BatchNormalization(axis=chanDim))
    model.add(MaxPooling2D(pool_size=(2, 2)))
    model.add(Dropout(0.25))

    model.add(Flatten())
    model.add(Dense(1024))
    model.add(Activation("relu"))
    model.add(BatchNormalization())
    model.add(Dropout(0.5))

    model.add(Dense(n_classes))
    model.add(Activation("softmax"))

    # Compile the model
    opt = Adam(learning_rate=init_lr, decay=init_lr / epochs)
    model.compile(loss="binary_crossentropy", optimizer=opt, metrics=["accuracy"])

    return model

# Define the SubsetDataGenerator class
class SubsetDataGenerator(tf.keras.utils.Sequence):
    """
    Custom data generator for a specific subset of data.
    """
    def __init__(self, image_files, labels, label_binarizer, batch_size=32, shuffle=True):
        self.image_files = image_files
        self.labels = labels
        self.label_binarizer = label_binarizer
        self.batch_size = batch_size
        self.shuffle = shuffle
        self.on_epoch_end()

    def __len__(self):
        return math.ceil(len(self.image_files) / self.batch_size)

    def __getitem__(self, index):
        indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]
        batch_image_files = [self.image_files[k] for k in indexes]
        batch_labels = [self.labels[k] for k in indexes]
        X, y = self.__data_generation(batch_image_files, batch_labels)
        return X, y

    def on_epoch_end(self):
        self.indexes = np.arange(len(self.image_files))
        if self.shuffle == True:
            np.random.shuffle(self.indexes)

    def __data_generation(self, batch_image_files, batch_labels):
        img_height, img_width, img_depth = 64, 64, 3
        X = np.empty((len(batch_image_files), img_height, img_width, img_depth), dtype=np.float16)
        y = np.empty((len(batch_image_files), len(self.label_binarizer.classes_)), dtype=np.float32)

        for i, (image_file, label) in enumerate(zip(batch_image_files, batch_labels)):
            image = cv2.imread(image_file)
            if image is not None:
                image = image.astype(np.float16) / 255.0
                X[i,] = image
                y[i,] = self.label_binarizer.transform([label])[0]
            else:
                print(f"Warning: Could not load image {image_file}. Filling with zeros.")
                X[i,] = np.zeros((img_height, img_width, img_depth), dtype=np.float16)
                y[i,] = np.zeros(len(self.label_binarizer.classes_), dtype=np.float32)
        return X, y


trained_models = []
input_shape = (height, width, depth) # Assuming height, width, and depth are defined

for i, (subset_image_files, subset_labels) in enumerate(split_datasets):
    print(f"[INFO] Training model {i+1} on subset {i+1}...")

    # Create training and validation data generators for the current subset
    # We will split the current subset into training and validation for this specific model
    train_subset_files, val_subset_files, train_subset_labels, val_subset_labels = train_test_split(
        subset_image_files, subset_labels, test_size=0.2, random_state=42, shuffle=True
    )

    train_generator_subset = SubsetDataGenerator(
        train_subset_files, train_subset_labels, label_binarizer, batch_size=BS, shuffle=True
    )

    validation_generator_subset = SubsetDataGenerator(
        val_subset_files, val_subset_labels, label_binarizer, batch_size=BS, shuffle=False
    )


    # Build a new CNN model for the current subset
    cnn_model_subset = build_cnn_model(input_shape, n_classes, INIT_LR, EPOCHS)
    print(f"[INFO] Model {i+1} architecture built.")

    # Define the accuracy callback for early stopping
    accuracy_callback = AccuracyThresholdCallback(acc_threshold=0.9, val_acc_threshold=0.9)

    # Train the model on the current subset
    print(f"[INFO] Training model {i+1}...")
    history_subset = cnn_model_subset.fit(
        train_generator_subset,
        validation_data=validation_generator_subset,
        epochs=EPOCHS,
        verbose=1,
        callbacks=[accuracy_callback]
    )
    print(f"[INFO] Training for model {i+1} completed.")

    # Store the trained model
    trained_models.append(cnn_model_subset)
    print(f"[INFO] Model {i+1} stored.")

print("[INFO] All models trained and stored.")

"""# Single Model - Attention CNN with 20% dataset
Train the attention_cnn_model on 20% of the augmented dataset and evaluate its performance.

**Reasoning**:
Load image paths and labels from the extracted directories and split them into training and validation sets.
"""

import os
from sklearn.model_selection import train_test_split
import numpy as np # Import numpy

extracted_base_dir = '/content/extracted_augmented_data/bb_random_augmented_dataset'
images_dir = os.path.join(extracted_base_dir, 'random_augmented_images')
labels_dir = os.path.join(extracted_base_dir, 'random_augmented_labels')

all_image_files = []
all_labels = []

label_files = sorted([f for f in os.listdir(labels_dir) if f.endswith('.txt')])
for label_file_name in label_files:
    label_file_path = os.path.join(labels_dir, label_file_name)
    with open(label_file_path, 'r') as f:
        for line in f:
            parts = line.strip().split(',')
            if len(parts) == 2:
                image_filename = parts[0]
                label = int(parts[1])
                all_image_files.append(os.path.join(images_dir, image_filename))
                all_labels.append(label)

print(f"[INFO] Total number of image file paths loaded: {len(all_image_files)}")

# Select a random 20% subset of the total data
subset_size = int(len(all_image_files) * 0.2)
# Use train_test_split to get a random subset (by splitting with test_size=1-0.2=0.8)
subset_image_files, _, subset_labels, _ = train_test_split(
    all_image_files, all_labels, test_size=0.8, random_state=42, shuffle=True
)

print(f"[INFO] Selected 20% subset size: {len(subset_image_files)}")

# Now split this 20% subset into training and validation sets (e.g., 80/20 split of the subset)
train_image_files, val_image_files, train_labels, val_labels = train_test_split(
    subset_image_files, subset_labels, test_size=0.2, random_state=42, shuffle=True
)


print(f"[INFO] Training set size (from 20% subset): {len(train_image_files)}")
print(f"  Approximate percentage of total data: {len(train_image_files) / len(all_image_files) * 100:.2f}%")
print(f"[INFO] Validation set size (from 20% subset): {len(val_image_files)}")
print(f"  Approximate percentage of total data: {len(val_image_files) / len(all_image_files) * 100:.2f}%")

train_labels[0]

from keras.models import Sequential
from keras.layers import Conv2D, MaxPooling2D, Activation, BatchNormalization, Dropout
from keras import backend as K

from keras.layers import Attention, GlobalAveragePooling2D, Dense, Input, Layer
import tensorflow as tf
from keras.models import Model
from tensorflow.keras.optimizers import Adam # Import Adam optimizer

# Define the CNN base model architecture (same as before)
cnn_base = Sequential()
inputShape = (height, width, depth) # Assuming height, width, depth are defined
chanDim = -1
if K.image_data_format() == "channels_first":
    inputShape = (depth, height, width)
    chanDim = 1

cnn_base.add(Conv2D(32, (3, 3), padding="same", input_shape=inputShape))
cnn_base.add(Activation("relu"))
cnn_base.add(BatchNormalization(axis=chanDim))
cnn_base.add(MaxPooling2D(pool_size=(3, 3)))
cnn_base.add(Dropout(0.25))

cnn_base.add(Conv2D(64, (3, 3), padding="same"))
cnn_base.add(Activation("relu"))
cnn_base.add(BatchNormalization(axis=chanDim))
cnn_base.add(Conv2D(64, (3, 3), padding="same"))
cnn_base.add(Activation("relu"))
cnn_base.add(BatchNormalization(axis=chanDim))
cnn_base.add(MaxPooling2D(pool_size=(2, 2)))
cnn_base.add(Dropout(0.25))

cnn_base.add(Conv2D(128, (3, 3), padding="same"))
cnn_base.add(Activation("relu"))
cnn_base.add(BatchNormalization(axis=chanDim))
cnn_base.add(Conv2D(128, (3, 3), padding="same"))
cnn_base.add(Activation("relu"))
cnn_base.add(BatchNormalization(axis=chanDim))
cnn_base.add(MaxPooling2D(pool_size=(2, 2)))
cnn_base.add(Dropout(0.25))

print("CNN base model created successfully.")
cnn_base.summary()

# Create a custom Keras Layer for reshaping (same as before)
@tf.keras.saving.register_keras_serializable() # Add this decorator
class ReshapeLayer(Layer):
    def __init__(self, target_shape, **kwargs):
        super(ReshapeLayer, self).__init__(**kwargs)
        self.target_shape = target_shape

    def call(self, inputs):
        return tf.reshape(inputs, self.target_shape)

# Create the attention CNN model (same as before)
inputs = Input(shape=(height, width, depth)) # Assuming height, width, depth are defined
cnn_output = cnn_base(inputs)
batch_size_tensor, h, w, c = cnn_output.shape # Use a different variable name for batch size

cnn_output_reshaped = ReshapeLayer(target_shape=(-1, h * w, c))(cnn_output)
attention_output = Attention()([cnn_output_reshaped, cnn_output_reshaped])
attention_output_reshaped_spatial = ReshapeLayer(target_shape=(-1, h, w, c))(attention_output)
x = GlobalAveragePooling2D()(attention_output_reshaped_spatial)
outputs = Dense(n_classes, activation='softmax')(x) # Assuming n_classes is defined

attention_cnn_model = Model(inputs=inputs, outputs=outputs)

print("CNN model with Attention layer created successfully.")
attention_cnn_model.summary()

# Compile the attention CNN model (using parameters from previous cells)
opt = Adam(learning_rate=INIT_LR, decay=INIT_LR / EPOCHS) # Assuming INIT_LR and EPOCHS are defined
attention_cnn_model.compile(loss="categorical_crossentropy", optimizer=opt, metrics=["accuracy"])

print("[INFO] Attention CNN model compiled.")

import tensorflow as tf
import numpy as np
import cv2
import math
from tensorflow.keras.callbacks import ModelCheckpoint # Import ModelCheckpoint

# Define a data generator for the subset data
class SubsetDataGenerator(tf.keras.utils.Sequence):
    """
    Custom data generator for a specific subset of data (train or validation).
    """
    def __init__(self, image_files, labels, label_binarizer, batch_size=32, shuffle=True):
        self.image_files = image_files
        self.labels = labels
        self.label_binarizer = label_binarizer
        self.batch_size = batch_size
        self.shuffle = shuffle
        self.on_epoch_end()

    def __len__(self):
        return math.ceil(len(self.image_files) / self.batch_size)

    def __getitem__(self, index):
        indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]
        batch_image_files = [self.image_files[k] for k in indexes]
        batch_labels = [self.labels[k] for k in indexes]
        X, y = self.__data_generation(batch_image_files, batch_labels)
        return X, y

    def on_epoch_end(self):
        self.indexes = np.arange(len(self.image_files))
        if self.shuffle == True:
            np.random.shuffle(self.indexes)

    def __data_generation(self, batch_image_files, batch_labels):
        # Assuming image shape is consistent
        img_height, img_width, img_depth = 64, 64, 3 # Use your defined image shape
        X = np.empty((len(batch_image_files), img_height, img_width, img_depth), dtype=np.float16)
        y = np.empty((len(batch_image_files), len(self.label_binarizer.classes_)), dtype=np.float32)

        for i, (image_file, label) in enumerate(zip(batch_image_files, batch_labels)):
            image = cv2.imread(image_file)
            if image is not None:
                image = image.astype(np.float16) / 255.0
                X[i,] = image
                #y[i,] = self.label_binarizer.transform([label])[0]
                y[i,] = self.label_binarizer.transform([label_binarizer.classes_[label]])[0]

            else:
                print(f"Warning: Could not load image {image_file}. Filling with zeros.")
                # Handle missing images
                X[i,] = np.zeros((img_height, img_width, img_depth), dtype=np.float16)
                y[i,] = np.zeros(len(self.label_binarizer.classes_), dtype=np.float32)
        return X, y

# Create data generators for the training and validation subsets
# Assumes train_image_files, val_image_files, train_labels, val_labels,
# label_binarizer, and BS are defined in previous cells.
if 'train_image_files' in locals() and 'val_image_files' in locals() and \
   'train_labels' in locals() and 'val_labels' in locals() and \
   'label_binarizer' in locals() and 'BS' in locals():

    train_subset_generator = SubsetDataGenerator(
        train_image_files, train_labels, label_binarizer, batch_size=BS, shuffle=True
    )
    print(f"[INFO] Training subset data generator created with {len(train_image_files)} images.")

    validation_subset_generator = SubsetDataGenerator(
        val_image_files, val_labels, label_binarizer, batch_size=BS, shuffle=False
    )
    print(f"[INFO] Validation subset data generator created with {len(val_image_files)} images.")

else:
    print("[ERROR] Required variables for data generators (train_image_files, etc.) not found.")


# Define the accuracy callback for early stopping (assuming AccuracyThresholdCallback is defined)
if 'AccuracyThresholdCallback' in locals():
    accuracy_callback = AccuracyThresholdCallback(acc_threshold=0.9, val_acc_threshold=0.9)
    print("[INFO] AccuracyThresholdCallback created.")
else:
    print("[ERROR] AccuracyThresholdCallback not found. Please run the cell that defines it.")
    accuracy_callback = None # Set to None if not defined

# Define ModelCheckpoint callback
checkpoint_filepath = '/tmp/checkpoint/best_model.h5' # Define checkpoint file path
model_checkpoint_callback = ModelCheckpoint(
    filepath=checkpoint_filepath,
    save_best_only=True, # Save only the best model based on the monitored metric
    monitor='val_accuracy', # Monitor validation accuracy
    mode='max', # Save when validation accuracy is maximum
    verbose=1
)
print(f"[INFO] ModelCheckpoint callback created. Best model will be saved to {checkpoint_filepath}")


# Train the attention CNN model using the subset data generators
# Assumes attention_cnn_model is defined and compiled
if 'attention_cnn_model' in locals() and train_subset_generator is not None and validation_subset_generator is not None:
    print("[INFO] Training attention CNN model on the 20% subset...")

    # Add the ModelCheckpoint callback to the callbacks list
    callbacks_list = [callback for callback in [accuracy_callback, model_checkpoint_callback] if callback is not None]

    history = attention_cnn_model.fit(
        train_subset_generator,
        validation_data=validation_subset_generator,
        epochs=EPOCHS, # Assuming EPOCHS is defined
        verbose=1,
        callbacks=callbacks_list # Use the updated callbacks list
    )
    print("[INFO] Attention CNN model training on subset completed.")

    # Save the final model after training
    final_model_filepath = 'attention_cnn_model_final.keras'
    attention_cnn_model.save(final_model_filepath)
    print(f"[INFO] Final model saved to {final_model_filepath}")

else:
    print("[ERROR] Attention CNN model or data generators not available for training.")

"""# Generate Feature Maps and Attention Heatmap
Show sample training images and labels and visualize feature maps of the attention_cnn_model.

## Display sample training images and labels

### Subtask:
Display a few sample images from the training subset along with their corresponding labels.

**Reasoning**:
Display a few sample images from the training subset along with their corresponding labels to visualize the data being used for training.
"""

import matplotlib.pyplot as plt
import cv2
import numpy as np

# Select a few sample images and labels from the training subset
num_samples_to_display = 5
sample_indices = np.random.choice(len(train_image_files), num_samples_to_display, replace=False)

plt.figure(figsize=(15, 5))

for i, idx in enumerate(sample_indices):
    image_file = train_image_files[idx]
    label_index = train_labels[idx] # Get the integer label index
    label = label_binarizer.classes_[label_index] # Convert index back to string label

    # Read and convert image
    image = cv2.imread(image_file)
    if image is not None:
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

        plt.subplot(1, num_samples_to_display, i + 1)
        plt.imshow(image)
        plt.title(label)
        plt.axis('off')
    else:
        print(f"Could not load image: {image_file}")


plt.tight_layout()
plt.show()

"""**Reasoning**:
The variable `train_image_files` was not defined in the current kernel. I need to re-run the code that defines it before I can use it to sample images for display.


"""

import os
from sklearn.model_selection import train_test_split
import numpy as np # Import numpy

extracted_base_dir = '/content/extracted_augmented_data/bb_random_augmented_dataset'
images_dir = os.path.join(extracted_base_dir, 'random_augmented_images')
labels_dir = os.path.join(extracted_base_dir, 'random_augmented_labels')

all_image_files = []
all_labels = []

label_files = sorted([f for f in os.listdir(labels_dir) if f.endswith('.txt')])
for label_file_name in label_files:
    label_file_path = os.path.join(labels_dir, label_file_name)
    with open(label_file_path, 'r') as f:
        for line in f:
            parts = line.strip().split(',')
            if len(parts) == 2:
                image_filename = parts[0]
                label = int(parts[1])
                all_image_files.append(os.path.join(images_dir, image_filename))
                all_labels.append(label)

print(f"[INFO] Total number of image file paths loaded: {len(all_image_files)}")

# Select a random 20% subset of the total data
subset_size = int(len(all_image_files) * 0.2)
# Use train_test_split to get a random subset (by splitting with test_size=1-0.2=0.8)
subset_image_files, _, subset_labels, _ = train_test_split(
    all_image_files, all_labels, test_size=0.8, random_state=42, shuffle=True
)

print(f"[INFO] Selected 20% subset size: {len(subset_image_files)}")

# Now split this 20% subset into training and validation sets (e.g., 80/20 split of the subset)
train_image_files, val_image_files, train_labels, val_labels = train_test_split(
    subset_image_files, subset_labels, test_size=0.2, random_state=42, shuffle=True
)


print(f"[INFO] Training set size (from 20% subset): {len(train_image_files)}")
print(f"  Approximate percentage of total data: {len(train_image_files) / len(all_image_files) * 100:.2f}%")
print(f"[INFO] Validation set size (from 20% subset): {len(val_image_files)}")
print(f"  Approximate percentage of total data: {len(val_image_files) / len(all_image_files) * 100:.2f}%")

import matplotlib.pyplot as plt
import cv2

# Select a few sample images and labels from the training subset
num_samples_to_display = 5
sample_indices = np.random.choice(len(train_image_files), num_samples_to_display, replace=False)

plt.figure(figsize=(15, 5))

for i, idx in enumerate(sample_indices):
    image_file = train_image_files[idx]
    label_index = train_labels[idx] # Get the integer label index
    label = label_binarizer.classes_[label_index] # Convert index back to string label

    # Read and convert image
    image = cv2.imread(image_file)
    if image is not None:
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

        plt.subplot(1, num_samples_to_display, i + 1)
        plt.imshow(image)
        plt.title(label)
        plt.axis('off')
    else:
        print(f"Could not load image: {image_file}")


plt.tight_layout()
plt.show()

"""## Prepare the model for feature map visualization

### Subtask:
Create a new Keras model that outputs the activations of intermediate layers of the `attention_cnn_model`.

**Reasoning**:
Define the input layer for the feature map model and iterate through the layers of the attention_cnn_model to collect their outputs.
"""

import tensorflow as tf
from keras.models import Model
from keras.layers import Input

# Get the input layer of the original attention_cnn_model
original_input = attention_cnn_model.input

# Get the outputs of all layers in the original attention_cnn_model
layer_outputs = [layer.output for layer in attention_cnn_model.layers]

# Create a new model that takes the original model's input and outputs the outputs of all layers
feature_map_model = Model(inputs=original_input, outputs=layer_outputs)

print("[INFO] Feature map model created successfully.")
# Note: Model summary might be very long as it includes all intermediate outputs
# feature_map_model.summary()

import random
import cv2
import numpy as np
import matplotlib.pyplot as plt

# Select a random index from the training image files
sample_image_index = random.randint(0, len(train_image_files) - 1)

# Get the file path and label for the selected sample image
sample_image_file = train_image_files[sample_image_index]
sample_label_index = train_labels[sample_image_index]
sample_label = label_binarizer.classes_[sample_label_index] # Convert index back to string label


print(f"[INFO] Selected sample image: {sample_image_file}")
print(f"[INFO] Corresponding label: {sample_label}")

# Load the sample image
sample_image = cv2.imread(sample_image_file)

if sample_image is not None:
    # Convert to RGB for displaying with matplotlib
    sample_image_rgb = cv2.cvtColor(sample_image, cv2.COLOR_BGR2RGB)

    # Display the original sample image
    plt.imshow(sample_image_rgb)
    plt.title(f"Sample Image: {sample_label}")
    plt.axis('off')
    plt.show()

    # Preprocess the image for the model (resize and normalize)
    # Assuming the model expects 64x64 images and float16 normalized data
    # You might need to adjust this based on your model's input requirements
    processed_sample_image = cv2.resize(sample_image, (width, height)) # Assuming width and height are defined
    processed_sample_image = processed_sample_image.astype(np.float16) / 255.0
    processed_sample_image = np.expand_dims(processed_sample_image, axis=0) # Add batch dimension


    print("[INFO] Sample image loaded and preprocessed for feature map visualization.")

else:
    print(f"[ERROR] Could not load sample image: {sample_image_file}")

# Predict the feature maps for the example image
# Assumes feature_map_model and processed_sample_image are defined in previous cells
if 'feature_map_model' in locals() and 'processed_sample_image' in locals():
    print("[INFO] Predicting feature maps...")
    feature_maps = feature_map_model.predict(processed_sample_image)
    print("[INFO] Feature map prediction completed.")

    # Visualize feature maps for a few layers
    # Assumes attention_cnn_model is defined
    layer_names = [layer.name for layer in attention_cnn_model.layers]

    images_per_row = 16 # Number of feature maps to display per row

    print("[INFO] Visualizing feature maps...")
    for layer_name, feature_map in zip(layer_names, feature_maps):
        # Do not plot the feature maps of layers that don't have a 4D output (e.g., Flatten, Dense)
        # Also handle the attention layer output which is 3D (batch, sequence_length, features)
        if feature_map.ndim not in [3, 4]:
            continue

        # Handle 4D convolutional/pooling layer outputs
        if feature_map.ndim == 4:
            # Number of features in the feature map
            n_features = feature_map.shape[-1]
            # The feature map has shape (1, size, size, n_features).
            size = feature_map.shape[1]

            # We will tile the activation channels in this matrix
            display_grid = np.zeros((size, images_per_row * size))

            # We'll tile each filter into this grid
            for col in range(min(n_features, images_per_row)): # Display up to images_per_row feature maps
                # Get a feature map from the current layer
                channel_image = feature_map[0, :, :, col]

                # Post-process the feature map to make it visually palatable
                # Avoid division by zero if std is zero
                if channel_image.std() != 0:
                    channel_image = (channel_image - channel_image.mean()) / channel_image.std()
                channel_image = channel_image * 64 + 128
                channel_image = np.clip(channel_image, 0, 255).astype('uint8')

                # Displays the grid
                display_grid[:, col * size : (col + 1) * size] = channel_image

        # Handle 3D attention layer output (batch, sequence_length, features)
        elif feature_map.ndim == 3 and layer_name == 'attention':
             # For visualization, treat sequence_length as a spatial dimension and features as channels
            sequence_length = feature_map.shape[1]
            n_features = feature_map.shape[2]

            # Determine a reasonable grid size for visualization
            # For simplicity, let's try to arrange the sequence elements in a near-square grid
            grid_size = int(np.ceil(np.sqrt(sequence_length)))
            display_grid_size = grid_size * images_per_row # Display a grid of grid_size x grid_size blocks

            display_grid = np.zeros((grid_size * grid_size, images_per_row * n_features)) # Adjust grid size for features

            # Iterate through sequence elements and display a few feature dimensions
            for i in range(min(sequence_length, grid_size * grid_size)):
                # Get the feature vector for the current sequence element
                feature_vector = feature_map[0, i, :]

                # Reshape the feature vector to a small spatial representation if needed,
                # or just display a portion of the features.
                # For now, let's display a portion of the features for a few sequence elements.

                # Simple approach: Tile a few feature dimensions
                for col in range(min(n_features, images_per_row)):
                     channel_value = feature_vector[col]

                     # Create a small block to represent this feature value
                     block_size = grid_size # Use grid_size for block size for visualization

                     # Post-process the feature value to make it visually palatable
                     # This is a simplified approach compared to processing a whole channel image
                     block_image = np.full((block_size, block_size), channel_value)

                     # Simple normalization and scaling
                     block_image = (block_image - np.mean(block_image)) / (np.std(block_image) if np.std(block_image) != 0 else 1e-5)
                     block_image = block_image * 64 + 128
                     block_image = np.clip(block_image, 0, 255).astype('uint8')


                     # Calculate position in the display grid
                     row_idx = i // grid_size
                     col_idx = i % grid_size

                     # Place the block in the display grid
                     # This tiling logic needs adjustment based on how we want to arrange the sequence elements
                     # Let's simplify and just display the feature vectors as rows of pixels

            # A more straightforward way to visualize the attention layer output as a "feature map"
            # is to reshape the sequence_length dimension back to the original spatial dimensions (e.g., 5x5)
            # if the original spatial dimensions are known and sequence_length is size*size.
            # Assuming the flattened spatial size before attention was 5x5 (from MaxPooling2D_2 output size)
            original_spatial_size = int(np.sqrt(sequence_length)) # Assuming sequence_length is a perfect square of the original spatial size

            if original_spatial_size * original_spatial_size == sequence_length:
                # Reshape to (batch, original_spatial_size, original_spatial_size, features)
                reshaped_attention_output = np.reshape(feature_map, (-1, original_spatial_size, original_spatial_size, n_features))

                # Now visualize this reshaped output like a normal 4D feature map
                size = original_spatial_size
                display_grid = np.zeros((size, images_per_row * size))

                for col in range(min(n_features, images_per_row)):
                     channel_image = reshaped_attention_output[0, :, :, col]

                     # Post-process
                     if channel_image.std() != 0:
                         channel_image = (channel_image - channel_image.mean()) / channel_image.std()
                     channel_image = channel_image * 64 + 128
                     channel_image = np.clip(channel_image, 0, 255).astype('uint8')

                     display_grid[:, col * size : (col + 1) * size] = channel_image

            else:
                 print(f"Warning: Could not reshape attention output of shape {feature_map.shape} to a square spatial grid for visualization.")
                 continue # Skip visualization for this layer if reshaping is not possible


        # Display the grid if it was created
        if 'display_grid' in locals() and display_grid.shape[0] > 0 and display_grid.shape[1] > 0:
             scale = 1. / size
             plt.figure(figsize=(scale * display_grid.shape[1],
                                scale * display_grid.shape[0]))
             plt.title(layer_name)
             plt.grid(False)
             plt.imshow(display_grid, aspect='auto', cmap='viridis')
             del display_grid # Clear display_grid for the next layer


    plt.show()

else:
    print("[ERROR] 'feature_map_model' or 'processed_sample_image' not found. Please run the preceding cells.")

"""# Task
Modify the model training code to make sure it stops when accuracy and validation accuracy is more than 90%.

## Modify the model to output attention weights

### Subtask:
Create a new Keras model or modify the existing `attention_cnn_model` to output the attention weights from the `Attention` layer in addition to the final output.

**Reasoning**:
Create a new Keras model that outputs the attention weights from the Attention layer in addition to the final output.
"""

from keras.models import Model

# Identify the Attention layer within the attention_cnn_model
attention_layer = None
for layer in attention_cnn_model.layers:
    if isinstance(layer, tf.keras.layers.Attention):
        attention_layer = layer
        break

if attention_layer:
    # Create a new Keras Model with multiple outputs
    attention_model_with_weights = Model(
        inputs=attention_cnn_model.input,
        outputs=[attention_cnn_model.output, attention_layer.output]
    )
    print("[INFO] Attention model with weights output created successfully.")
    # You can optionally print the summary to verify the outputs
    # attention_model_with_weights.summary()
else:
    print("[ERROR] Attention layer not found in the attention_cnn_model.")
    attention_model_with_weights = None

"""## Get attention weights for the sample image

### Subtask:
Use the modified model to predict the attention weights for the selected sample image.

**Reasoning**:
Use the modified model to predict the attention weights for the selected sample image.
"""

# 1. Check if the attention_model_with_weights and processed_sample_image variables exist.
if 'attention_model_with_weights' in locals() and 'processed_sample_image' in locals():
    print("[INFO] Required variables found. Predicting attention weights...")

    # 2. Use the .predict() method of attention_model_with_weights with processed_sample_image as input.
    predictions = attention_model_with_weights.predict(processed_sample_image)

    # 3. The prediction will return a list of outputs; the second element in this list will be the attention weights. Store these attention weights in a variable.
    # The output of the model is [final_output, attention_output]
    sample_attention_weights = predictions[1]

    # 4. Print a confirmation message indicating that the attention weights were obtained.
    print("[INFO] Attention weights obtained successfully.")
    print(f"Shape of attention weights: {sample_attention_weights.shape}")

else:
    # 5. If the required variables do not exist, print an error message.
    print("[ERROR] Required variables 'attention_model_with_weights' or 'processed_sample_image' not found.")

"""## Process attention weights

### Subtask:
Reshape and potentially aggregate the attention weights to correspond to the spatial dimensions of the original image.

**Reasoning**:
Reshape and aggregate the attention weights to correspond to the spatial dimensions of the original image.
"""

import numpy as np
import math

# 1. Check if the sample_attention_weights variable exists.
if 'sample_attention_weights' not in locals():
    print("[ERROR] 'sample_attention_weights' not found. Please run the preceding cells.")
else:
    print("[INFO] 'sample_attention_weights' found. Proceeding with reshaping and aggregation.")

    # 2. Get the original spatial size of the feature maps before the attention layer.
    # The shape of the input to the Attention layer was (batch_size, sequence_length, features)
    # In our case, the sequence_length is sample_attention_weights.shape[1]
    sequence_length = sample_attention_weights.shape[1]
    original_spatial_size = int(np.sqrt(sequence_length))

    # 3. Check if original_spatial_size squared is equal to sequence_length.
    if original_spatial_size * original_spatial_size != sequence_length:
        print(f"[WARNING] Sequence length ({sequence_length}) is not a perfect square of the calculated spatial size ({original_spatial_size}). Reshaping to spatial dimensions may not be accurate.")
        # Depending on the task requirements, you might stop here or proceed with a different visualization approach
        # For this subtask, we will proceed as requested, but acknowledge the potential inaccuracy.

    # 4. Reshape sample_attention_weights from (batch_size, sequence_length, features) to (batch_size, original_spatial_size, original_spatial_size, features).
    # We have a batch size of 1 for a single sample.
    batch_size = sample_attention_weights.shape[0]
    features = sample_attention_weights.shape[2]
    spatial_attention_weights = np.reshape(sample_attention_weights, (batch_size, original_spatial_size, original_spatial_size, features))
    print(f"[INFO] Reshaped attention weights to spatial dimensions. New shape: {spatial_attention_weights.shape}")


    # 5. Aggregate the attention weights across the feature dimension.
    # Using np.mean() to get a single attention value per spatial location.
    aggregated_attention_map = np.mean(spatial_attention_weights, axis=-1)
    print(f"[INFO] Aggregated attention weights across features. New shape: {aggregated_attention_map.shape}")

    # 6. Remove the batch dimension from aggregated_attention_map.
    # Since batch_size is 1, we can simply squeeze the first dimension.
    final_attention_map = np.squeeze(aggregated_attention_map, axis=0)

    # 7. Print the shape of final_attention_map.
    print(f"[INFO] Final attention map created. Shape: {final_attention_map.shape}")

"""**Reasoning**:
Visualize the final aggregated attention map to understand which spatial regions the model focused on for the sample image.


"""

import matplotlib.pyplot as plt

# Check if the final_attention_map variable exists
if 'final_attention_map' not in locals():
    print("[ERROR] 'final_attention_map' not found. Please run the preceding cells.")
else:
    print("[INFO] 'final_attention_map' found. Visualizing attention map.")

    # Visualize the attention map
    plt.figure(figsize=(6, 6))
    plt.imshow(final_attention_map, cmap='viridis') # Using 'viridis' cmap as in previous visualizations
    plt.title('Aggregated Attention Map')
    plt.colorbar(label='Attention Weight (Mean)')
    plt.axis('off') # Hide axes ticks and labels
    plt.show()

    # Optional: Overlay the attention map on the original image for better context
    # This requires resizing the attention map to the original image dimensions.
    # Assumes sample_image_rgb is available from previous steps

    if 'sample_image_rgb' in locals() and sample_image_rgb is not None:
        print("[INFO] sample_image_rgb found. Attempting to overlay attention map.")
        # Resize the attention map to the original image dimensions
        # Use interpolation to smooth the resized map
        resized_attention_map = cv2.resize(final_attention_map, (sample_image_rgb.shape[1], sample_image_rgb.shape[0]), interpolation=cv2.INTER_CUBIC)

        # Normalize the resized attention map to [0, 1] for alpha blending
        resized_attention_map_normalized = (resized_attention_map - np.min(resized_attention_map)) / (np.max(resized_attention_map) - np.min(resized_attention_map))

        # Create a heatmap from the normalized attention map
        heatmap = cv2.applyColorMap(np.uint8(255 * resized_attention_map_normalized), cv2.COLORMAP_VIRIDIS)
        heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB) # Convert BGR to RGB


        # Blend the heatmap with the original image
        # Use the normalized attention map as the alpha channel for blending
        # Ensure both images are of the same data type and shape (except for channels for original image)
        # Convert original image and heatmap to float for blending
        original_image_float = sample_image_rgb.astype(np.float32) / 255.0
        heatmap_float = heatmap.astype(np.float32) / 255.0

        # Expand the normalized attention map to 3 channels to match image shape
        alpha = np.expand_dims(resized_attention_map_normalized, axis=-1)

        # Perform alpha blending: output = (1 - alpha) * original + alpha * heatmap
        overlay_image = (1 - alpha) * original_image_float + alpha * heatmap_float
        overlay_image = np.clip(overlay_image, 0, 1) # Clip values to be within [0, 1]


        # Display the overlayed image
        plt.figure(figsize=(8, 8))
        plt.imshow(overlay_image)
        plt.title(f'Attention Map Overlay on Original Image ({sample_label})')
        plt.axis('off')
        plt.show()

    else:
        print("[INFO] sample_image_rgb not found. Skipping attention map overlay.")

"""## Summary:

### Data Analysis Key Findings

*   A new Keras model was successfully created to output attention weights from the `Attention` layer in addition to the final prediction.
*   The attention weights for a sample image were successfully extracted using the modified model, resulting in a shape of (1, 25, 128).
*   The attention weights were reshaped from a flattened sequence to a spatial grid of shape (1, 5, 5, 128).
*   The attention weights were aggregated across the feature dimension using the mean, resulting in a 2D attention map with a shape of (5, 5).
*   The final 2D attention map was successfully visualized as a heatmap and overlaid on the original image.

### Insights or Next Steps

*   The ability to visualize attention weights provides valuable insight into which parts of the input image the model focuses on when making a prediction.
*   This attention visualization can be used to debug the model, understand its decision-making process, and potentially identify areas for improvement in the model architecture or training data.

"""

import numpy as np
from collections import Counter

# Assumes all_labels and label_binarizer are defined in previous cells

if 'all_labels' in locals() and 'label_binarizer' in locals():
    # Get unique labels and their counts
    label_counts = Counter(all_labels)

    print("[INFO] Label distribution in the augmented dataset:")

    # Get the mapping from integer label to string label
    label_map = {i: name for i, name in enumerate(label_binarizer.classes_)}

    # Display each label and its count
    for label_index, count in sorted(label_counts.items()):
        label_name = label_map.get(label_index, f"Unknown Label ({label_index})")
        print(f"- {label_name}: {count}")

    print(f"\nTotal number of images in all_labels: {len(all_labels)}")

else:
    print("[ERROR] 'all_labels' or 'label_binarizer' not found. Please run the cells that define them.")

"""# Resnet model
Change the model training code to make sure it stops when accuracy and validation accuracy is more than 90%

## Load a pre-trained resnet base model

### Subtask:
Import a pre-trained ResNet model (e.g., ResNet50) from Keras Applications, excluding the top classification layer.

**Reasoning**:
Import a pre-trained ResNet model (ResNet50) from Keras Applications, excluding the top classification layer, and instantiate it with ImageNet weights and the specified input shape.
"""

from tensorflow.keras.applications import ResNet50

# Define the input shape
input_shape = (height, width, depth) # Assuming height, width, and depth are defined

# Load the pre-trained ResNet50 model, excluding the top classification layer
resnet_base = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)

print("[INFO] Pre-trained ResNet50 base model loaded successfully.")

"""## Modify the resnet base model for the custom dataset

### Subtask:
Add new layers on top of the ResNet base model to adapt it for your plant disease classification task (e.g., flatten the output and add dense layers with the correct number of output classes).

**Reasoning**:
Add flatten and dense layers on top of the ResNet base model to adapt it for the plant disease classification task.
"""

from keras.layers import Flatten, Dense
from keras.models import Model
from keras.layers import Input

# Get the output of the resnet_base model
input_tensor = Input(shape=input_shape)
resnet_output = resnet_base(input_tensor)

# Flatten the output of the resnet_base model
flatten_layer = Flatten()(resnet_output)

# Add a Dense layer with a suitable number of units and a ReLU activation function
dense_layer_1 = Dense(1024, activation='relu')(flatten_layer)

# Add another Dense layer with n_classes units and a softmax activation function
output_layer = Dense(n_classes, activation='softmax')(dense_layer_1)

# Create a new Keras Model
resnet_model = Model(inputs=input_tensor, outputs=output_layer)

print("[INFO] ResNet model with added layers created successfully.")

# Optionally, print the summary of the new model
resnet_model.summary()

"""## Compile the modified resnet model

### Subtask:
Compile the new ResNet-based model with an appropriate optimizer, loss function, and metrics.

**Reasoning**:
Compile the ResNet-based model with the Adam optimizer and binary crossentropy loss.
"""

from tensorflow.keras.optimizers import Adam

# Compile the resnet_model
opt = Adam(learning_rate=INIT_LR) # Decay is ignored in newer Keras versions
# Change loss function to categorical_crossentropy for multi-class classification
resnet_model.compile(loss="categorical_crossentropy", optimizer=opt, metrics=["accuracy"])

print("[INFO] ResNet model compiled successfully with categorical_crossentropy.")

import tensorflow as tf
import numpy as np
import cv2
import math
from tensorflow.keras.callbacks import ModelCheckpoint, Callback
from tensorflow.keras.utils import to_categorical
from sklearn.model_selection import train_test_split # Import train_test_split
import random # Import random for shuffling lists

# Define the AccuracyThresholdCallback class if not already defined
if 'AccuracyThresholdCallback' not in locals():
    class AccuracyThresholdCallback(Callback):
        def __init__(self, acc_threshold=0.9, val_acc_threshold=0.9):
            super(AccuracyThresholdCallback, self).__init__()
            self.acc_threshold = acc_threshold
            self.val_acc_threshold = val_acc_threshold

        def on_epoch_end(self, epoch, logs=None):
            if logs is None:
                logs = {}
            acc = logs.get('accuracy')
            val_acc = logs.get('val_accuracy')
            if acc is not None and val_acc is not None:
                if acc >= self.acc_threshold and val_acc >= self.val_acc_threshold:
                    print(f"\nStopping training as accuracy ({acc:.4f}) and validation accuracy ({val_acc:.4f}) reached the threshold.")
                    self.model.stop_training = True

# Define the SubsetDataGenerator class if not already defined
if 'SubsetDataGenerator' not in locals():
    class SubsetDataGenerator(tf.keras.utils.Sequence):
        """
        Custom data generator for a specific subset of data (train or validation).
        """
        def __init__(self, image_files, labels, label_binarizer, batch_size=32, shuffle=True):
            self.image_files = image_files
            self.labels = labels # These are integer labels now
            self.label_binarizer = label_binarizer
            self.batch_size = batch_size
            self.shuffle = shuffle

            # --- Modification Start ---
            # Shuffle image_files and labels together after loading them
            combined_data = list(zip(self.image_files, self.labels))
            random.shuffle(combined_data)
            self.image_files, self.labels = zip(*combined_data)
            self.image_files = list(self.image_files)
            self.labels = list(self.labels)
            # --- Modification End ---



            self.on_epoch_end() # Initial shuffle of indexes


        def __len__(self):
            """Denotes the number of batches per epoch."""
            return math.ceil(len(self.image_files) / self.batch_size)

        def __getitem__(self, index):
            """Generate one batch of data."""
            # Generate indexes of the batch
            indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]


            # Find list of IDs for the batch
            batch_image_files = [self.image_files[k] for k in indexes]
            batch_labels = [self.labels[k] for k in indexes]

            # Generate data
            X, y = self.__data_generation(batch_image_files, batch_labels)

            return X, y

        def on_epoch_end(self):
            """Updates indexes after each epoch."""
            self.indexes = np.arange(len(self.image_files))
            if self.shuffle == True:
                np.random.shuffle(self.indexes)

        def __data_generation(self, batch_image_files, batch_labels):
            """Generates data containing batch_size samples."""
            # Initialization
            # Assuming image shape is consistent
            img_height, img_width, img_depth = 64, 64, 3 # Use your defined image shape
            X = np.empty((len(batch_image_files), img_height, img_width, img_depth), dtype=np.float16)
            y = np.empty((len(batch_image_files), len(self.label_binarizer.classes_)), dtype=np.float32) # Use num_classes for shape

            #print(f"\n--- Inisde getitem Image ---")
            for i, (image_file, label) in enumerate(zip(batch_image_files, batch_labels)):
                image = cv2.imread(image_file)
                if image is not None:
                    image = image.astype(np.float16) / 255.0
                    X[i,] = image

                    # Convert label to one-hot encoding
                    # Use label_binarizer.transform for consistency with how it was fitted
                    #y[i,] = self.label_binarizer.transform([label])[0]
                    y[i,] = self.label_binarizer.transform([label_binarizer.classes_[label]])[0]


                    #print(f"\n--- Inisde getitem Image {image_file} ---")
                    #print(f"\n--- Inisde getitem label {label} ---")
                    #print(f"\n--- Inisde getitem one-hot label {y[i,]} ---")

                else:
                    print(f"Warning: Could not load image {image_file}. Filling with zeros.")
                    # Handle missing images
                    X[i,] = np.zeros((img_height, img_width, img_depth), dtype=np.float16)
                    y[i,] = np.zeros(len(self.label_binarizer.classes_), dtype=np.float32) # Fill with zero vector for labels


            return X, y


# Create data generators for the training and validation subsets if not already created
# Assumes train_image_files, val_image_files, train_labels, val_labels,
# label_binarizer, and BS are defined in previous cells.
if 'train_subset_generator' not in locals() or 'validation_subset_generator' not in locals():
    if 'train_image_files' in locals() and 'val_image_files' in locals() and \
       'train_labels' in locals() and 'val_labels' in locals() and \
       'label_binarizer' in locals() and 'BS' in locals():

        train_subset_generator = SubsetDataGenerator(
            train_image_files, train_labels, label_binarizer, batch_size=BS, shuffle=True
        )
        print(f"[INFO] Training subset data generator created with {len(train_image_files)} images.")

        validation_subset_generator = SubsetDataGenerator(
            val_image_files, val_labels, label_binarizer, batch_size=BS, shuffle=False
        )
        print(f"[INFO] Validation subset data generator created with {len(val_image_files)} images.")

    else:
        print("[ERROR] Required variables for data generators (train_image_files, etc.) not found.")
        train_subset_generator = None
        validation_subset_generator = None


# Define the accuracy callback for early stopping
accuracy_callback = AccuracyThresholdCallback(acc_threshold=0.9, val_acc_threshold=0.9)
print("[INFO] AccuracyThresholdCallback created.")

# Define ModelCheckpoint callback
checkpoint_filepath = '/tmp/resnet_best_model_20percent.keras' # Define checkpoint file path
model_checkpoint_callback = ModelCheckpoint(
    filepath=checkpoint_filepath,
    save_best_only=True, # Save only the best model based on the monitored metric
    monitor='val_accuracy', # Monitor validation accuracy
    mode='max', # Save when validation accuracy is maximum
    verbose=1
)
print(f"[INFO] ModelCheckpoint callback created. Best model will be saved to {checkpoint_filepath}")


# Train the resnet_model using the 20% subset data generators
# Assumes resnet_model is defined and compiled
if 'resnet_model' in locals() and train_subset_generator is not None and validation_subset_generator is not None:
    print("[INFO] Training ResNet model on the 20% subset...")

    # Create a list of callbacks
    callbacks_list = [callback for callback in [accuracy_callback, model_checkpoint_callback] if callback is not None]

    # Train the model
    resnet_history = resnet_model.fit(
        train_subset_generator,
        validation_data=validation_subset_generator,
        epochs=EPOCHS, # Assuming EPOCHS is defined
        verbose=1,
        callbacks=callbacks_list
    )
    print("[INFO] ResNet model training on 20% subset completed.")

else:
    print("[ERROR] ResNet model or data generators not available for training.")

"""# E. Vision Transfomer Model
Train a Vision Transformer model on the provided dataset for disease classification using the existing data generators.

## Define vision transformer model

### Subtask:
Define the Vision Transformer model architecture, including the patch creation, patch embedding, and the Transformer blocks.

**Reasoning**:
Define the custom Keras layers for patch extraction and patch embedding as well as the Transformer block function and then assemble them into the Vision Transformer model architecture.
"""

import tensorflow as tf
from tensorflow.keras.layers import Input, Dense, Dropout, LayerNormalization, MultiHeadAttention, Add, GlobalAveragePooling2D, Reshape
from tensorflow.keras import layers

# 1. Define the PatchExtractor layer
class PatchExtractor(layers.Layer):
    def __init__(self, patch_size, **kwargs):
        super().__init__(**kwargs)
        self.patch_size = patch_size

    def call(self, images):
        batch_size = tf.shape(images)[0]
        patches = tf.image.extract_patches(
            images=images,
            sizes=[1, self.patch_size, self.patch_size, 1],
            strides=[1, self.patch_size, self.patch_size, 1],
            rates=[1, 1, 1, 1],
            padding="VALID",
        )
        patch_dims = patches.shape[-1]
        patches = tf.reshape(patches, [batch_size, -1, patch_dims])
        return patches

# 2. Define the PatchEmbedding layer
class PatchEmbedding(layers.Layer):
    def __init__(self, num_patches, projection_dim, **kwargs):
        super().__init__(**kwargs)
        self.num_patches = num_patches
        self.projection = layers.Dense(units=projection_dim)
        self.position_embedding = layers.Embedding(
            input_dim=num_patches, output_dim=projection_dim
        )

    def call(self, patch):
        positions = tf.range(start=0, limit=self.num_patches, delta=1)
        encoded = self.projection(patch) + self.position_embedding(positions)
        return encoded

# 3. Define the TransformerBlock function
def TransformerBlock(encoded_patches, num_heads, mlp_ratio, dropout_rate, projection_dim):
    # Layer normalization 1
    x1 = LayerNormalization(epsilon=1e-6)(encoded_patches)
    # Multi-head attention
    attention_output = MultiHeadAttention(
        num_heads=num_heads, key_dim=projection_dim, dropout=dropout_rate
    )(x1, x1)
    # Skip connection 1
    x2 = Add()([attention_output, encoded_patches])
    # Layer normalization 2
    x3 = LayerNormalization(epsilon=1e-6)(x2)
    # MLP
    x4 = Dense(units=projection_dim * mlp_ratio, activation=tf.nn.gelu)(x3)
    x4 = Dropout(dropout_rate)(x4)
    x4 = Dense(units=projection_dim)(x4)
    # Skip connection 2
    return Add()([x4, x3])

# 4. Define the Vision Transformer model
def build_vit_model(input_shape, n_classes, patch_size, num_transformer_blocks, num_heads, mlp_ratio, projection_dim, dropout_rate):
    inputs = Input(shape=input_shape)
    # Create patches
    patches = PatchExtractor(patch_size)(inputs)
    # Encode patches
    num_patches = (input_shape[0] // patch_size) * (input_shape[1] // patch_size)
    encoded_patches = PatchEmbedding(num_patches, projection_dim)(patches)

    # Create Transformer blocks
    for _ in range(num_transformer_blocks):
        encoded_patches = TransformerBlock(
            encoded_patches, num_heads, mlp_ratio, dropout_rate, projection_dim
        )

    # Global average pooling and classification head
    representation = LayerNormalization(epsilon=1e-6)(encoded_patches)
    representation = GlobalAveragePooling2D()(tf.expand_dims(representation, 1)) # Add spatial dimension for pooling
    representation = Dropout(dropout_rate)(representation)
    logits = Dense(n_classes, activation="softmax")(representation)

    # Create the model
    model = tf.keras.Model(inputs=inputs, outputs=logits)
    return model

print("[INFO] Vision Transformer model architecture defined.")

"""## Compile vision transformer model

### Subtask:
Compile the Vision Transformer model with an appropriate optimizer and loss function.

**Reasoning**:
Compile the Vision Transformer model with the Adam optimizer and categorical crossentropy loss.
"""

import tensorflow as tf
from tensorflow.keras.layers import Input, Dense, Dropout, LayerNormalization, MultiHeadAttention, Add, GlobalAveragePooling2D, Reshape
from tensorflow.keras import layers
from tensorflow.keras.optimizers import Adam
from keras.models import Model # Import Model

# Define the PatchExtractor layer (assuming it's defined in a previous cell)
# class PatchExtractor(layers.Layer):
#     ...

# Define the PatchEmbedding layer (assuming it's defined in a previous cell)
# class PatchEmbedding(layers.Layer):
#     ...

# Define the TransformerBlock function (assuming it's defined in a previous cell)
# def TransformerBlock(...):
#     ...

# Define a custom Keras Layer to wrap tf.expand_dims
class ExpandDimsLayer(layers.Layer):
    def __init__(self, axis, **kwargs):
        super().__init__(**kwargs)
        self.axis = axis

    def call(self, inputs):
        return tf.expand_dims(inputs, axis=self.axis)

    def get_config(self):
        config = super().get_config()
        config.update({'axis': self.axis})
        return config


# Define the Vision Transformer model
def build_vit_model(input_shape, n_classes, patch_size, num_transformer_blocks, num_heads, mlp_ratio, projection_dim, dropout_rate):
    inputs = Input(shape=input_shape)
    # Create patches
    patches = PatchExtractor(patch_size)(inputs)
    # Encode patches
    num_patches = (input_shape[0] // patch_size) * (input_shape[1] // patch_size)
    encoded_patches = PatchEmbedding(num_patches, projection_dim)(patches)

    # Create Transformer blocks
    for _ in range(num_transformer_blocks):
        encoded_patches = TransformerBlock(
            encoded_patches, num_heads, mlp_ratio, dropout_rate, projection_dim
        )

    # Global average pooling and classification head
    representation = LayerNormalization(epsilon=1e-6)(encoded_patches)
    # Use the custom ExpandDimsLayer instead of tf.expand_dims directly
    representation = ExpandDimsLayer(axis=1)(representation)
    representation = GlobalAveragePooling2D()(representation)
    representation = Dropout(dropout_rate)(representation)
    logits = Dense(n_classes, activation="softmax")(representation)

    # Create the model
    model = tf.keras.Model(inputs=inputs, outputs=logits)
    return model

# Define ViT model parameters (adjust as needed based on your dataset and computational resources)
patch_size = 8 # Example patch size
num_transformer_blocks = 4 # Example number of Transformer blocks
num_heads = 4 # Example number of attention heads
mlp_ratio = 4 # Example MLP ratio
projection_dim = 128 # Example projection dimension
dropout_rate = 0.1 # Example dropout rate

# Define input_shape and n_classes assuming height, width, depth and label_binarizer are defined
if 'height' in locals() and 'width' in locals() and 'depth' in locals() and 'label_binarizer' in locals():
    input_shape = (height, width, depth)
    n_classes = len(label_binarizer.classes_)
    print(f"[INFO] Defined input_shape: {input_shape}")
    print(f"[INFO] Defined n_classes: {n_classes}")

    # Build the ViT model
    vit_model = build_vit_model(
        input_shape,
        n_classes,
        patch_size,
        num_transformer_blocks,
        num_heads,
        mlp_ratio,
        projection_dim,
        dropout_rate
    )
    print("[INFO] Vision Transformer model built.")

    # 1. Define an Adam optimizer with a suitable learning rate
    opt = Adam(learning_rate=INIT_LR) # Assuming INIT_LR is defined

    # 2. Compile the vit_model
    vit_model.compile(loss="categorical_crossentropy", optimizer=opt, metrics=["accuracy"])

    # 3. Print a confirmation message
    print("[INFO] Vision Transformer model compiled.")
else:
    print("[ERROR] Required variables for input_shape or n_classes not found. Cannot build and compile ViT model.")
    vit_model = None # Set to None if model cannot be built

"""## Train vision transformer model

### Subtask:
Train the Vision Transformer model using the existing `SubsetDataGenerator` for training and validation data.

**Reasoning**:
Train the Vision Transformer model using the existing data generators and callbacks.
"""

import tensorflow as tf
import numpy as np
import os
import cv2
import math
from sklearn.model_selection import train_test_split # Import train_test_split

class SplitAugmentedDataGenerator(tf.keras.utils.Sequence):
    """
    Custom data generator that loads augmented images and labels in batches
    and performs train/validation split internally based on file paths.
    """
    def __init__(self, extracted_base_dir, label_binarizer, batch_size=32, shuffle=True, validation_split=0.2, is_validation=False):
        self.extracted_base_dir = extracted_base_dir
        self.label_binarizer = label_binarizer
        self.batch_size = batch_size
        self.shuffle = shuffle
        self.validation_split = validation_split
        self.is_validation = is_validation

        self.extracted_images_dir = os.path.join(self.extracted_base_dir, 'random_augmented_images')
        self.extracted_labels_dir = os.path.join(self.extracted_base_dir, 'random_augmented_labels')

        # Load all image file paths and corresponding labels from the label files
        all_image_files = []
        all_labels = []

        label_files = sorted([f for f in os.listdir(self.extracted_labels_dir) if f.endswith('.txt')])
        for label_file_name in label_files:
            label_file_path = os.path.join(self.extracted_labels_dir, label_file_name)
            with open(label_file_path, 'r') as f:
                for line in f:
                    parts = line.strip().split(',')
                    if len(parts) == 2:
                        image_filename = parts[0]
                        label = int(parts[1])
                        all_image_files.append(os.path.join(self.extracted_images_dir, image_filename))
                        all_labels.append(label)

        # Perform the train/validation split on the file paths and labels
        # We use a fixed random state to ensure the split is the same for both generators
        if not is_validation:
            self.image_files, _, self.labels, _ = train_test_split(
                all_image_files, all_labels, test_size=validation_split, random_state=42, shuffle=self.shuffle
            )
        else:
            _, self.image_files, _, self.labels = train_test_split(
                all_image_files, all_labels, test_size=validation_split, random_state=42, shuffle=self.shuffle
            )


        self.on_epoch_end() # Initial shuffling if needed

    def __len__(self):
        """Denotes the number of batches per epoch."""
        return math.ceil(len(self.image_files) / self.batch_size)

    def __getitem__(self, index):
        """Generate one batch of data."""
        # Generate indexes of the batch
        indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]

        # Find list of IDs for the batch
        batch_image_files = [self.image_files[k] for k in indexes]
        batch_labels = [self.labels[k] for k in indexes]

        # Generate data
        X, y = self.__data_generation(batch_image_files, batch_labels)

        return X, y

    def on_epoch_end(self):
        """Updates indexes after each epoch."""
        self.indexes = np.arange(len(self.image_files))
        if self.shuffle == True:
            np.random.shuffle(self.indexes)

    def __data_generation(self, batch_image_files, batch_labels):
        """Generates data containing batch_size samples."""
        # Initialization
        # Assuming all images have the same dimensions (e.g., 64x64x3)
        img_height, img_width, img_depth = 64, 64, 3 # Use your defined image shape
        X = np.empty((len(batch_image_files), img_height, img_width, img_depth), dtype=np.float16)
        y = np.empty((len(batch_image_files), len(self.label_binarizer.classes_)), dtype=np.float32) # Use float32 for one-hot encoding

        # Generate data
        for i, (image_file, label) in enumerate(zip(batch_image_files, batch_labels)):
            # Load image
            image = cv2.imread(image_file)
            if image is not None:
                # Convert image to float and normalize
                image = image.astype(np.float16) / 255.0
                X[i,] = image

                # Convert label to one-hot encoding
                #y[i,] = self.label_binarizer.transform([label])[0]
                y[i,] = self.label_binarizer.transform([label_binarizer.classes_[label]])[0]

            else:
                print(f"Warning: Could not load image {image_file} in data generator. Filling with zeros.")
                # Handle missing images - fill with zeros or a placeholder
                X[i,] = np.zeros((img_height, img_width, img_depth), dtype=np.float16)
                y[i,] = np.zeros(len(self.label_binarizer.classes_), dtype=np.float32)


        return X, y

# --- Example Usage ---
# Assuming you have extracted the zip file to '/content/extracted_augmented_data'
# and have the label_binarizer object available from previous steps

# Define the base directory where the zip extracted its contents
extracted_base_dir = '/content/extracted_augmented_data/bb_random_augmented_dataset' # Use the directory containing image and label folders

# Make sure 'label_binarizer' and 'BS' (batch size) are defined in previous cells

if 'label_binarizer' in locals() and 'BS' in locals():
    # Create the training data generator
    train_generator = SplitAugmentedDataGenerator(
        extracted_base_dir=extracted_base_dir,
        label_binarizer=label_binarizer,
        batch_size=BS,
        shuffle=True,
        validation_split=0.2, # Define your desired validation split
        is_validation=False
    )
    print("[INFO] Training data generator created.")
    print(f"Number of training batches per epoch: {len(train_generator)}")
    print(f"Total number of training images: {len(train_generator.image_files)}")


    # Create the validation data generator
    validation_generator = SplitAugmentedDataGenerator(
        extracted_base_dir=extracted_base_dir,
        label_binarizer=label_binarizer,
        batch_size=BS,
        shuffle=False, # No need to shuffle validation data
        validation_split=0.2, # Use the same validation split
        is_validation=True # Set to True for the validation generator
    )
    print("[INFO] Validation data generator created.")
    print(f"Number of validation batches per epoch: {len(validation_generator)}")
    print(f"Total number of validation images: {len(validation_generator.image_files)}")

else:
    print("[ERROR] 'label_binarizer' or 'BS' not found. Please run the cells that define them.")

# Define the accuracy callback for early stopping
accuracy_callback = AccuracyThresholdCallback(acc_threshold=0.9, val_acc_threshold=0.9)
print("[INFO] AccuracyThresholdCallback created.")

# Define ModelCheckpoint callback
checkpoint_filepath = '/tmp/checkpoint/best_model.h5' # Define checkpoint file path
model_checkpoint_callback = ModelCheckpoint(
    filepath=checkpoint_filepath,
    save_best_only=True, # Save only the best model based on the monitored metric
    monitor='val_accuracy', # Monitor validation accuracy
    mode='max', # Save when validation accuracy is maximum
    verbose=1
)
print(f"[INFO] ModelCheckpoint callback created. Best model will be saved to {checkpoint_filepath}")


# Train the attention CNN model using the subset data generators
# Assumes attention_cnn_model is defined and compiled
if 'vit_model' in locals() and train_generator is not None and validation_generator is not None:
    print("[INFO] Training  VIT  model on the entire subset...")

    # Add the ModelCheckpoint callback to the callbacks list
    callbacks_list = [callback for callback in [accuracy_callback, model_checkpoint_callback] if callback is not None]

    history = vit_model.fit(
        train_generator,
        validation_data=validation_generator,
        epochs=EPOCHS, # Assuming EPOCHS is defined
        verbose=1,
        callbacks=callbacks_list # Use the updated callbacks list
    )
    print("[INFO] VIT  model training on subset completed.")

    # Save the final model after training
    final_model_filepath = 'resnet_cnn_model_final.keras'
    resnet_model.save(final_model_filepath)
    print(f"[INFO] Final model saved to {final_model_filepath}")

else:
    print("[ERROR] VIT  model or data generators not available for training.")

"""# F. Inception Net model
Train an InceptionNet model with transfer learning on the plant disease dataset using the existing data generators and evaluate its performance.

## Load pre-trained inceptionnet model

### Subtask:
Load a pre-trained InceptionNet model (e.g., InceptionV3) from Keras Applications, excluding the top classification layer.

**Reasoning**:
Import InceptionV3 and load the pre-trained model without the top layer.
"""

# Define the input shape with minimum required dimensions for InceptionV3
input_shape = (75, 75, depth) # Minimum size is 75x75 for InceptionV3

# Load the pre-trained InceptionV3 model, excluding the top classification layer
inception_base = InceptionV3(weights='imagenet', include_top=False, input_shape=input_shape)

print("[INFO] Pre-trained InceptionV3 base model loaded successfully with updated input shape.")

"""## Modify inceptionnet model

### Subtask:
Add new layers on top of the InceptionNet base model to adapt it for your plant disease classification task (e.g., flatten the output and add dense layers with the correct number of output classes).

**Reasoning**:
Add flatten and dense layers on top of the InceptionNet base model to adapt it for the plant disease classification task.
"""

from keras.layers import Flatten, Dense
from keras.models import Model
from keras.layers import Input

# Get the output of the inception_base model
input_tensor = Input(shape=input_shape)
inception_output = inception_base(input_tensor)

# Flatten the output of the inception_base model
flatten_layer = Flatten()(inception_output)

# Add a Dense layer with a suitable number of units and a ReLU activation function
dense_layer_1 = Dense(1024, activation='relu')(flatten_layer)

# Add another Dense layer with n_classes units and a softmax activation function
output_layer = Dense(n_classes, activation='softmax')(dense_layer_1)

# Create a new Keras Model
inception_model = Model(inputs=input_tensor, outputs=output_layer)

print("[INFO] InceptionNet model with added layers created successfully.")

# Optionally, print the summary of the new model
inception_model.summary()

"""**Reasoning**:
Compile the InceptionNet model with the Adam optimizer and categorical crossentropy loss.


"""

from tensorflow.keras.optimizers import Adam

# Compile the inception_model
opt = Adam(learning_rate=INIT_LR) # Decay is ignored in newer Keras versions
inception_model.compile(loss="categorical_crossentropy", optimizer=opt, metrics=["accuracy"])

print("[INFO] InceptionNet model compiled successfully.")

"""**Reasoning**:
Train the InceptionNet model using the existing data generators and callbacks.


"""

import tensorflow as tf
import numpy as np
import cv2
import math
from tensorflow.keras.callbacks import ModelCheckpoint, Callback
from tensorflow.keras.utils import to_categorical
from sklearn.model_selection import train_test_split # Import train_test_split
import random # Import random for shuffling lists

# Define the AccuracyThresholdCallback class if not already defined
if 'AccuracyThresholdCallback' not in locals():
    class AccuracyThresholdCallback(Callback):
        def __init__(self, acc_threshold=0.9, val_acc_threshold=0.9):
            super(AccuracyThresholdCallback, self).__init__()
            self.acc_threshold = acc_threshold
            self.val_acc_threshold = val_acc_threshold

        def on_epoch_end(self, epoch, logs=None):
            if logs is None:
                logs = {}
            acc = logs.get('accuracy')
            val_acc = logs.get('val_accuracy')
            if acc is not None and val_acc is not None:
                if acc >= self.acc_threshold and val_acc >= self.val_acc_threshold:
                    print(f"\nStopping training as accuracy ({acc:.4f}) and validation accuracy ({val_acc:.4f}) reached the threshold.")
                    self.model.stop_training = True

# Define the SubsetDataGenerator class, including target_size in __init__
class SubsetDataGenerator(tf.keras.utils.Sequence):
    """
    Custom data generator for a specific subset of data (train or validation).
    """
    def __init__(self, image_files, labels, label_binarizer, batch_size=32, shuffle=True, target_size=(64, 64)):
        self.image_files = image_files
        self.labels = labels # These are integer labels now
        self.label_binarizer = label_binarizer
        self.batch_size = batch_size
        self.shuffle = shuffle
        self.target_size = target_size # Store target_size

        # Shuffle image_files and labels together after loading them
        combined_data = list(zip(self.image_files, self.labels))
        random.shuffle(combined_data)
        self.image_files, self.labels = zip(*combined_data)
        self.image_files = list(self.image_files)
        self.labels = list(self.labels)

        self.on_epoch_end() # Initial shuffle of indexes


    def __len__(self):
        """Denotes the number of batches per epoch."""
        return math.ceil(len(self.image_files) / self.batch_size)

    def __getitem__(self, index):
        """Generate one batch of data."""
        # Generate indexes of the batch
        indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]

        # Find list of IDs for the batch
        batch_image_files = [self.image_files[k] for k in indexes]
        batch_labels = [self.labels[k] for k in indexes]

        # Generate data
        X, y = self.__data_generation(batch_image_files, batch_labels)

        return X, y

    def on_epoch_end(self):
        """Updates indexes after each epoch."""
        self.indexes = np.arange(len(self.image_files))
        if self.shuffle == True:
            np.random.shuffle(self.indexes)

    def __data_generation(self, batch_image_files, batch_labels):
        """Generates data containing batch_size samples."""
        # Initialization
        # Assuming image shape is consistent
        img_height, img_width = self.target_size # Use target_size for reshaping
        img_depth = 3
        X = np.empty((len(batch_image_files), img_height, img_width, img_depth), dtype=np.float16)
        y = np.empty((len(batch_image_files), len(self.label_binarizer.classes_)), dtype=np.float32) # Use num_classes for shape

        for i, (image_file, label) in enumerate(zip(batch_image_files, batch_labels)):
            image = cv2.imread(image_file)
            if image is not None:
                # Resize image to target size
                image = cv2.resize(image, (img_width, img_height))
                image = image.astype(np.float16) / 255.0
                X[i,] = image

                # Convert label to one-hot encoding
                if label < len(self.label_binarizer.classes_):
                     y[i,] = self.label_binarizer.transform([self.label_binarizer.classes_[label]])[0]
                else:
                     print(f"Warning: Label index {label} out of bounds for label_binarizer classes. Filling with zeros.")
                     y[i,] = np.zeros(len(self.label_binarizer.classes_), dtype=np.float32)


            else:
                print(f"Warning: Could not load image {image_file}. Filling with zeros.")
                # Handle missing images
                X[i,] = np.zeros((img_height, img_width, img_depth), dtype=np.float16)
                y[i,] = np.zeros(len(self.label_binarizer.classes_), dtype=np.float32) # Fill with zero vector for labels


        return X, y

# Define the base directory where the zip extracted its contents
extracted_base_dir = '/content/extracted_augmented_data/bb_random_augmented_dataset' # Use the directory containing image and label folders
images_dir = os.path.join(extracted_base_dir, 'random_augmented_images')
labels_dir = os.path.join(extracted_base_dir, 'random_augmented_labels')


# Load all image file paths and corresponding labels from the label files
all_image_files = []
all_labels = []

label_files = sorted([f for f in os.listdir(labels_dir) if f.endswith('.txt')])
for label_file_name in label_files:
    label_file_path = os.path.join(labels_dir, label_file_name)
    with open(label_file_path, 'r') as f:
        for line in f:
            parts = line.strip().split(',')
            if len(parts) == 2:
                image_filename = parts[0]
                label = int(parts[1])
                all_image_files.append(os.path.join(images_dir, image_filename))
                all_labels.append(label)

print(f"[INFO] Total number of image file paths loaded: {len(all_image_files)}")

# Select a random 20% subset of the total data
subset_size = int(len(all_image_files) * 0.2)
# Use train_test_split to get a random subset (by splitting with test_size=1-0.8=0.2)
subset_image_files, _, subset_labels, _ = train_test_split(
    all_image_files, all_labels, test_size=0.8, random_state=42, shuffle=True
)


print(f"[INFO] Selected 20% subset size: {len(subset_image_files)}")

# Now split this 20% subset into training and validation sets (e.g., 80/20 split of the subset)
train_image_files, val_image_files, train_labels, val_labels = train_test_split(
    subset_image_files, subset_labels, test_size=0.2, random_state=42, shuffle=True
)


print(f"[INFO] Training set size (from 20% subset): {len(train_image_files)}")
print(f"  Approximate percentage of total data: {len(train_image_files) / len(all_image_files) * 100:.2f}%")
print(f"[INFO] Validation set size (from 20% subset): {len(val_image_files)}")
print(f"  Approximate percentage of total data: {len(val_image_files) / len(all_image_files) * 100:.2f}%")


# Create data generators for the training and validation subsets
# Assumes label_binarizer and BS are defined in previous cells.
# Use the updated input_shape for target_size
if 'label_binarizer' in locals() and 'BS' in locals() and 'input_shape' in locals():

    train_subset_generator = SubsetDataGenerator(
        train_image_files, train_labels, label_binarizer, batch_size=BS, shuffle=True, target_size=(input_shape[0], input_shape[1])
    )
    print(f"[INFO] Training subset data generator created with {len(train_image_files)} images and target size {input_shape[0]}x{input_shape[1]}.")


    validation_subset_generator = SubsetDataGenerator(
        val_image_files, val_labels, label_binarizer, batch_size=BS, shuffle=False, target_size=(input_shape[0], input_shape[1])
    )
    print(f"[INFO] Validation subset data generator created with {len(val_image_files)} images and target size {input_shape[0]}x{input_shape[1]}.")

else:
    print("[ERROR] Required variables for data generators (label_binarizer, BS, input_shape) not found.")
    train_subset_generator = None
    validation_subset_generator = None


# Define the accuracy callback for early stopping
accuracy_callback = AccuracyThresholdCallback(acc_threshold=0.9, val_acc_threshold=0.9)
print("[INFO] AccuracyThresholdCallback created.")

# Define ModelCheckpoint callback
checkpoint_filepath = '/tmp/inceptionnet_best_model_20percent.keras' # Define checkpoint file path
model_checkpoint_callback = ModelCheckpoint(
    filepath=checkpoint_filepath,
    save_best_only=True, # Save only the best model based on the monitored metric
    monitor='val_accuracy', # Monitor validation accuracy
    mode='max', # Save when validation accuracy is maximum
    verbose=1
)
print(f"[INFO] ModelCheckpoint callback created. Best InceptionNet model will be saved to {checkpoint_filepath}")


# Train the inception_model using the 20% subset data generators
# Assumes inception_model is defined and compiled
if 'inception_model' in locals() and train_subset_generator is not None and validation_subset_generator is not None:
    print("[INFO] Training InceptionNet model on the 20% subset...")

    # Create a list of callbacks
    callbacks_list = [callback for callback in [accuracy_callback, model_checkpoint_callback] if callback is not None]

    # Train the model
    inception_history = inception_model.fit(
        train_subset_generator,
        validation_data=validation_subset_generator,
        epochs=EPOCHS, # Assuming EPOCHS is defined
        verbose=1,
        callbacks=callbacks_list
    )
    print("[INFO] InceptionNet model training on 20% subset completed.")

else:
    print("[ERROR] InceptionNet model or data generators not available for training.")