#!/usr/bin/env python3
"""
Test script to demonstrate enhanced LangGraph node metrics
"""

import asyncio
import time
import sys
import os
sys.path.append(os.getcwd())

from observability.metrics import get_metrics

async def test_enhanced_metrics():
    """Test the enhanced metrics functionality"""
    
    print("üöÄ Testing Enhanced LangGraph Node Metrics")
    print("=" * 50)
    
    metrics = get_metrics()
    if not metrics.is_initialized():
        print("‚ùå Metrics not initialized!")
        return
    
    print("‚úÖ Metrics system initialized")
    
    # Test 1: Basic node execution metrics
    print("\nüìä Test 1: Basic Node Execution Metrics")
    metrics.record_node_execution("test_initial", 0.15, "success", "test_session", {"workflow_step": "1"})
    metrics.record_node_execution("test_classifying", 2.34, "success", "test_session", {"has_ml": "true"})
    metrics.record_node_execution("test_prescribing", 1.87, "success", "test_session", {"has_recommendation": "true"})
    metrics.record_node_execution("test_completed", 0.05, "success", "test_session")
    print("   ‚úì Recorded node executions with enhanced labels")
    
    # Test 2: Node transitions  
    print("\nüîÑ Test 2: Node Transitions")
    metrics.record_node_transition("initial", "classifying", "test_session")
    metrics.record_node_transition("classifying", "prescribing", "test_session")
    metrics.record_node_transition("prescribing", "completed", "test_session")
    print("   ‚úì Recorded workflow transitions")
    
    # Test 3: Node input/output metrics
    print("\nüì• Test 3: Node Input/Output Metrics")
    metrics.record_node_input_metrics("classifying", has_image=True, message_length=145, has_context=True, context_keys=3)
    metrics.record_node_input_metrics("prescribing", has_image=False, message_length=67, has_context=True, context_keys=5)
    print("   ‚úì Recorded input characteristics")
    
    metrics.record_node_output_metrics("classifying", output_length=890, messages_generated=2, tools_used=1, has_classification=True)
    metrics.record_node_output_metrics("prescribing", output_length=1245, messages_generated=3, tools_used=1, has_prescription=True)
    print("   ‚úì Recorded output characteristics")
    
    # Test 4: Tool usage metrics
    print("\nüõ†Ô∏è Test 4: Tool Usage Metrics")
    metrics.record_node_tool_usage("classifying", "classification_tool", 2.1, True)
    metrics.record_node_tool_usage("prescribing", "prescription_tool", 1.6, True)
    metrics.record_node_tool_usage("show_vendors", "vendor_tool", 0.8, True)
    metrics.record_node_tool_usage("insurance", "insurance_tool", 3.2, True)
    print("   ‚úì Recorded tool usage with durations")
    
    # Test 5: Workflow progression
    print("\nüìà Test 5: Workflow Progression")
    metrics.record_node_state_progression("initial", state_complexity=8, workflow_step=1, is_retry=False)
    metrics.record_node_state_progression("classifying", state_complexity=12, workflow_step=2, is_retry=False)
    metrics.record_node_state_progression("prescribing", state_complexity=18, workflow_step=3, is_retry=False)
    metrics.record_node_state_progression("completed", state_complexity=20, workflow_step=4, is_retry=False)
    print("   ‚úì Recorded workflow progression")
    
    # Test 6: Simulate some variety
    print("\nüé≠ Test 6: Simulate Workflow Variety")
    
    # Different node types and outcomes
    for i in range(3):
        session = f"test_variety_{i}"
        
        # Vary the workflow paths
        if i == 0:
            # Normal workflow
            metrics.record_node_execution("initial", 0.12, "success", session)
            metrics.record_node_execution("classifying", 1.95, "success", session)
            metrics.record_node_execution("prescribing", 1.43, "success", session)
            metrics.record_node_execution("completed", 0.03, "success", session)
            metrics.record_node_tool_usage("classifying", "classification_tool", 1.8, True)
            
        elif i == 1:
            # Insurance workflow
            metrics.record_node_execution("initial", 0.08, "success", session)
            metrics.record_node_execution("insurance", 4.2, "success", session)
            metrics.record_node_execution("completed", 0.02, "success", session)
            metrics.record_node_tool_usage("insurance", "insurance_tool", 4.0, True)
            
        else:
            # Error workflow
            metrics.record_node_execution("initial", 0.15, "success", session)
            metrics.record_node_execution("classifying", 0.5, "error", session)
            metrics.record_node_execution("error", 0.1, "success", session)
            metrics.record_node_tool_usage("classifying", "classification_tool", 0.45, False)
    
    print("   ‚úì Simulated diverse workflow scenarios")
    
    print(f"\nüéâ Enhanced metrics test completed!")
    print(f"üìä Check your Grafana dashboards:")
    print(f"   ‚Ä¢ LangGraph Node Analytics: http://localhost:3000/d/langgraph-nodes")
    print(f"   ‚Ä¢ ML Performance: http://localhost:3000/d/sasya-ml-performance") 
    print(f"   ‚Ä¢ System Overview: http://localhost:3000/d/sasya-engine-overview")
    print(f"\nüîç Or query Prometheus directly: http://localhost:9090")
    
    # Show some key metrics to query
    print(f"\nüìà Key Metrics to Explore:")
    print(f"   ‚Ä¢ node_executions_total - Node execution counts by type")
    print(f"   ‚Ä¢ node_execution_duration_seconds - Node execution times")
    print(f"   ‚Ä¢ node_transitions_total - Workflow transitions")
    print(f"   ‚Ä¢ node_tool_usage_total - Tool usage by node and tool")
    print(f"   ‚Ä¢ node_tool_duration_seconds - Tool execution times")
    print(f"   ‚Ä¢ workflow_progression_total - Workflow phase progression")
    
if __name__ == "__main__":
    asyncio.run(test_enhanced_metrics())
